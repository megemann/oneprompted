# V1 Dataset for Prompt Engineering Fine-Tuning

## Overview
This dataset was created to train a model for improving user prompts through prompt engineering techniques. It consists of pairs of "bad" (original) and "good" (improved) prompts, along with metadata about prompt types and complexity levels.

## Dataset Characteristics
- **Size**: Approximately 1,450 prompt pairs
- **Source**: AI-generated examples using Gemini 2.0 Flash

## Complexity Distribution
- **Medium complexity**: 50% (most common real-world case)
- **Low complexity**: 25% (simpler tasks)
- **High complexity**: 25% (challenging scenarios)

## Prompt Type Distribution
Based on common usage patterns in AI interactions:

### Higher frequency (10-15% each):
- INFORMATIONAL (12%)
- QUESTION_ANSWERING (12%)
- INSTRUCTIONAL (10%)
- SUMMARIZATION (10%)

### Medium frequency (5-10% each):
- CREATIVE_WRITING (8%)
- PROGRAMMING_CODE_GENERATION (8%)
- ANALYSIS_CRITIQUE (7%)
- CONVERSATIONAL (7%)
- CLASSIFICATION_TAGGING (6%)

### Lower frequency (1-5% each):
- COMPARISON (5%)
- COMPLETION (4%)
- STYLE_TONE_CHANGE (4%)
- DATA_EXTRACTION (3%)
- TRANSLATION (2%)
- OTHER (2%)

## Dataset Structure
Each entry contains:
- task_description: Brief description of the task
- complexity: Difficulty level (low, medium, high)
- bad_prompt: Original, suboptimal prompt
- good_prompt: Improved, well-engineered prompt
- expected_answer: Description of ideal response
- prompting_techniques: List of techniques used in the improved prompt
- prompt_type: Category of the prompt
- notes: Additional information (optional)

## Prompting Techniques
The dataset showcases various prompting techniques including:
- ROLE_PROMPTING
- CHAIN_OF_THOUGHT
- ONE_SHOT_FEW_SHOT
- TREE_OF_THOUGHTS
- CODE_PROMPTING
- CONTEXTUAL_PROMPTING
- SELF_CONSISTENCY
- SYSTEM_PROMPTING
- STEP_BACK_PROMPTING
- GENERAL_ZERO_SHOT

