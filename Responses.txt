Step 6
Responses notebook:
    Goal- Gather all improved prompts and responses for all models for later evaluation

    Setup - 
        - Load test data, and connect free API_KEY as well as a tier 1 for payment

    Dataset - 
        - For testing purposes, we used the sampled and cleaned testing set from 'Prompt_Training_2.0'

    Specifications - 
        - When refering to the 'base untuned model', it corresponds to 'gemini-2.0-flash-lite-001', which is the compact but fine-tunable model available in vertex AI
        - When refering to the 'Gemini model', it corresponds to 'gemini-2.0-flash', commonly available via the google api 

    Prompt Improvements:
        - To gather an accurate sample of the data, there are 4 classes I wanted to evaluate:
            1. No model Response: Control, no model improvement
            2. Base Model Responses: Feed the original prompt from the testing set into the base untuned model
            3. Fine Tuned V1 Responses: Feed the original prompt into the intial fine tuned model
                Prompt for both 2 and 3:
                    f'''You are a prompt engineering expert that transforms simple 
                    prompts into more effective versions. Analyze the input prompt and create
                    an improved version that includes specific details, context, desired output format, 
                    and any relevant constraints. Make the prompt clear, specific, and designed to 
                    generate high-quality responses.
                    
                    Input Prompt: {prompt}

                    Respond ONLY with the text of the improved prompt, without any explanations, 
                    introductions, or additional commentary.
                    '''
            4. Fine Tuned V2 Responses: Feed the original seperated instruction and context into the updated fine tuned model
                Prompt for 4:
                    f'''You are a prompt engineering expert.

                    Your task is to rewrite the instruction below using advanced prompt engineering techniques. 
                    If context is provided, use it as *background knowledge* to better understand the task â€” but do not include it in the final output.

                    Guidelines:
                    - Enhance the instruction to be clearer, more specific, and more effective
                    - Use any prompting technique that best fits
                    - Ground your rewrite in the provided context, if applicable
                    - Do NOT copy or reference the context in your rewritten instruction

                    Context:
                    {context}

                    Original Instruction:
                    {instruction}

                    Output ONLY the improved instruction without any additional text, explanations, or acknowledgments.
                    Improved Instruction:
                    '''
            DISCLAIMER: Due to training differences, these models are prompted differently, which could indirectly lead to variance.
            However, the V2 of the fine tuned model is refined differently, and will take input differently, so it is neccesary even though it could disrupt the integrity of the study
            NOTE: the overall text fed into the models is the same, the ONLY difference is that the context was manually seperated as to be given different discrestion, while the 
            other models recieved the entire prompt as a big block. When experimenting with this, we found an issue in the neccesary context getting altered by the LLM, resulting
            in this solution

    Outputs:
        1. No model response: Simply prompt Gemini with the 'original prompt' from the dataset
        2-3: Fine Tuned V1 and Base Model: Prompt with the improved prompt and record the response
        4: Fine Tuned V2 response: Prompt with the context appended onto the end of the fine tuned instruction.
