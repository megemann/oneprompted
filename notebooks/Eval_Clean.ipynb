{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f9f7f7",
   "metadata": {},
   "source": [
    "## 1  Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob, json, re, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "# install pyarrow if Parquet needed\n",
    "from pathlib import Path\n",
    "import pandas as pd, joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import google.generativeai as genai, os, time\n",
    "from google.api_core import retry, exceptions\n",
    "\n",
    "\n",
    "with open('../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n",
    "    \n",
    "os.environ[\"GOOGLE_API_KEY\"] = env_vars[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "#env.json\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "model.generate_content = retry.Retry(predicate=lambda e: hasattr(e, 'code') and e.code in {429, 503})(model.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853a6d9",
   "metadata": {},
   "source": [
    "## 2  Train (or load) the logistic‑regression rubric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3484df",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = Path(\"train_logreg.joblib\")\n",
    "DATA_FILE  = Path(\"../data/Prompt_Training_2.0/cleaned_labeled_train.csv\")\n",
    "\n",
    "if MODEL_FILE.exists():\n",
    "    # load the already‐trained model\n",
    "    bundle = joblib.load(MODEL_FILE)\n",
    "    tfidf, clf = bundle['tfidf'], bundle['clf']\n",
    "else:\n",
    "    # load your dual‐column CSV\n",
    "    df_raw = pd.read_csv(DATA_FILE)\n",
    "\n",
    "    # original prompts = label 0 (bad)\n",
    "    neg = (\n",
    "      df_raw[['original_prompt']]\n",
    "        .rename(columns={'original_prompt':'text'})\n",
    "        .assign(label=0)\n",
    "    )\n",
    "    # improved prompts = label 1 (good)\n",
    "    pos = (\n",
    "      df_raw[['cleaned_improved_prompt']]\n",
    "        .rename(columns={'cleaned_improved_prompt':'text'})\n",
    "        .assign(label=1)\n",
    "    )\n",
    "    # combine into one training set\n",
    "    df_train = pd.concat([neg, pos], ignore_index=True)\n",
    "    \n",
    "    # train TF‑IDF + LogReg\n",
    "    tfidf = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "    X = tfidf.fit_transform(df_train['text'])\n",
    "    clf = LogisticRegression(max_iter=2000).fit(X, df_train['label'])\n",
    "    \n",
    "    # save for future runs\n",
    "    joblib.dump({'tfidf': tfidf, 'clf': clf}, MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382f7ed",
   "metadata": {},
   "source": [
    "## 3  Embedding helper (optional semantic guard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a7c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ↓ Replace with your own embedding backend (OpenAI, Gemini, HF, etc.)\n",
    "def get_embedding(text:str):\n",
    "    raise NotImplementedError(\"Plug in your embedding API call here\")\n",
    "\n",
    "def cosine(a,b):\n",
    "    a,b=np.array(a),np.array(b)\n",
    "    return float(np.dot(a,b)/(np.linalg.norm(a)+1e-9)/(np.linalg.norm(b)+1e-9))\n",
    "\n",
    "def semantic_ok(orig:str, cand:str, threshold:float=0.85)->bool:\n",
    "    try:\n",
    "        return cosine(get_embedding(orig), get_embedding(cand)) >= threshold\n",
    "    except NotImplementedError:\n",
    "        # If you haven't wired embeddings yet, skip the semantic check.\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d9805",
   "metadata": {},
   "source": [
    "## 4  Load model CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV_DIR = Path('../data')   # change if needed\n",
    "FILE_MAP = {                               # filename → user‑friendly tag\n",
    "    'BaseModelResponses/responses.csv'   : 'none',\n",
    "    'FineTunedV1Responses/responses.csv': 'v1',\n",
    "    'FineTunedV2Responses/responses.csv': 'v2',\n",
    "    'NoModelResponses/responses.csv'     : 'base',\n",
    "}\n",
    "\n",
    "frames = []\n",
    "for fname, tag in FILE_MAP.items():\n",
    "    path = CSV_DIR / fname\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_csv(path)[['original_prompt', 'response']].copy()\n",
    "    df = df.rename(columns={'response': f'resp_{tag}'})\n",
    "    frames.append(df)\n",
    "\n",
    "# Outer‑merge on original_prompt so we keep prompts that appear in any file\n",
    "base = frames[0]\n",
    "for f in frames[1:]:\n",
    "    base = base.merge(f, on='original_prompt', how='outer')\n",
    "\n",
    "df_merged = base\n",
    "print('Merged shape:', df_merged.shape)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effb27c",
   "metadata": {},
   "source": [
    "## 5  Evaluate each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6abcef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── One‑time train & dump ──\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_raw = pd.read_csv(\"../data/Prompt_Training_2.0/cleaned_labeled_train.csv\")\n",
    "neg = df_raw[[\"original_prompt\"]].rename(columns={\"original_prompt\":\"text\"}).assign(label=0)\n",
    "pos = df_raw[[\"cleaned_improved_prompt\"]].rename(columns={\"cleaned_improved_prompt\":\"text\"}).assign(label=1)\n",
    "df_train = pd.concat([neg, pos], ignore_index=True)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "X = tfidf.fit_transform(df_train[\"text\"])\n",
    "clf = LogisticRegression(max_iter=2000).fit(X, df_train[\"label\"])\n",
    "\n",
    "joblib.dump({\"tfidf\":tfidf, \"clf\":clf}, MODEL_FILE)\n",
    "print(\"Trained & saved\", MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d57623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load your trained TF‑IDF + LogisticRegression rubric model ──\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_FILE = Path(\"train_logreg.joblib\")\n",
    "if not MODEL_FILE.exists():\n",
    "    raise FileNotFoundError(f\"{MODEL_FILE} not found — run your training cell first.\")\n",
    "\n",
    "bundle = joblib.load(MODEL_FILE)\n",
    "tfidf, clf = bundle[\"tfidf\"], bundle[\"clf\"]\n",
    "\n",
    "def grade_prompt_logreg(prompt: str) -> float:\n",
    "    \"\"\"\n",
    "    Returns the probability that 'prompt' is a 'good' prompt,\n",
    "    as learned from your labeled prompt vs improved_prompt data.\n",
    "    \"\"\"\n",
    "    vec = tfidf.transform([prompt])        # shape (1, n_features)\n",
    "    return float(clf.predict_proba(vec)[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_cols = [c for c in df_merged.columns if c.startswith('resp_')]\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df_merged.iterrows(), total=len(df_merged)):\n",
    "    orig = row['original_prompt']\n",
    "    for col in variant_cols:\n",
    "        cand = row[col]\n",
    "        if pd.isna(cand):\n",
    "            continue\n",
    "        # optional: drop if it drifts semantically\n",
    "        if not semantic_ok(orig, cand):\n",
    "            continue\n",
    "        score = grade_prompt_logreg(cand)\n",
    "        results.append({\n",
    "            'original_prompt': orig,\n",
    "            'model': col.replace('resp_', ''),\n",
    "            'score': score\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print('Scored', len(results_df), 'prompt‑variant pairs')\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d79ed5",
   "metadata": {},
   "source": [
    "## 6  Aggregate & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = (\n",
    "    results_df\n",
    "    .groupby('model')['score']\n",
    "    .agg(['mean','count'])\n",
    "    .sort_values('mean', ascending=False)\n",
    ")\n",
    "print(agg)\n",
    "\n",
    "#  ▶ choose the best model per prompt\n",
    "best_by_prompt = (\n",
    "    results_df\n",
    "    .sort_values('score', ascending=False)\n",
    "    .drop_duplicates('original_prompt')    # use prompt instead of conversation_id\n",
    "    .rename(columns={'score':'best_score'})\n",
    ")\n",
    "best_by_prompt['rank'] = 1                  # every retained row is the #1 pick\n",
    "best_by_prompt = best_by_prompt[\n",
    "    ['original_prompt','model','best_score','rank']\n",
    "]\n",
    "print(\"\\nBest variant per prompt:\")\n",
    "display(best_by_prompt.head())\n",
    "\n",
    "#  ▶ write out full scored table\n",
    "results_df.to_csv('../data/PromptEval/prompt_eval_scores.csv', index=False)\n",
    "print('\\nSaved scores → prompt_eval_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978663fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Rubric evaluator (uses your existing `model`) ──\n",
    "import json\n",
    "\n",
    "rubric_instructions = '''\n",
    "***STRICT FORMAT INSTRUCTIONS***  \n",
    "Your response **must** be **only** a single JSON object matching exactly the schema below.  \n",
    "Do **NOT** include any prose, bullet points, headings, or explanation outside the JSON.  \n",
    "If you cannot comply, return an empty JSON object `{}`.\n",
    "\n",
    "Schema:\n",
    "{\n",
    "  \"task_category\": \"<Informational|Creative|Technical|Problem-solving|Analytical>\",\n",
    "  \"relevance\": 1,\n",
    "  \"accuracy\": 1,\n",
    "  \"completeness\": 1,\n",
    "  \"clarity\": 1,\n",
    "  \"creativity\": 1,\n",
    "  \"conciseness\": 1,\n",
    "  \"technical_correctness\": 1,\n",
    "  \"actionability\": 1,\n",
    "  \"weighted_score\": 0.0\n",
    "}\n",
    "\n",
    "Calculations:\n",
    "- Informational: relevance*0.25 + accuracy*0.3 + completeness*0.2 + clarity*0.15 + conciseness*0.1  \n",
    "- Creative:      relevance*0.2 + completeness*0.15 + clarity*0.15 + creativity*0.4 + conciseness*0.1  \n",
    "- Technical:     relevance*0.15 + accuracy*0.2 + completeness*0.15 + clarity*0.15 + technical_correctness*0.25 + actionability*0.1  \n",
    "- Problem-solving: relevance*0.2 + accuracy*0.15 + completeness*0.15 + clarity*0.1 + actionability*0.4  \n",
    "- Analytical:    relevance*0.2 + accuracy*0.25 + completeness*0.2 + clarity*0.15 + creativity*0.2  \n",
    "\n",
    "Example output:\n",
    "{\n",
    "  \"task_category\": \"Technical\",\n",
    "  \"relevance\": 5,\n",
    "  \"accuracy\": 4,\n",
    "  \"completeness\": 5,\n",
    "  \"clarity\": 4,\n",
    "  \"creativity\": 3,\n",
    "  \"conciseness\": 4,\n",
    "  \"technical_correctness\": 5,\n",
    "  \"actionability\": 5,\n",
    "  \"weighted_score\": 4.6\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "def evaluate_with_rubric(prompt: str,\n",
    "                         response: str,\n",
    "                         pause: float = 0.2) -> dict:\n",
    "    payload = f\"\"\"{rubric_instructions}\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Model Response:\n",
    "{response}\n",
    "\"\"\"\n",
    "    try:\n",
    "        out = model.generate_content(payload)\n",
    "        text = out.text.strip()\n",
    "        # Print raw for debugging (once)\n",
    "        # print(\"RAW RUBRIC REPLY:\", text[:200])\n",
    "\n",
    "        # Find the first {...} in the output\n",
    "        m = re.search(r'\\{[\\s\\S]*?\\}', text)\n",
    "        if not m:\n",
    "            # no JSON found at all\n",
    "            return {}\n",
    "        raw = m.group()\n",
    "        return json.loads(raw)\n",
    "\n",
    "    except exceptions.GoogleAPICallError as e:\n",
    "        print(\"Gemini API error:\", e)\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Failed to parse JSON:\", raw, e)\n",
    "        return {}\n",
    "    finally:\n",
    "        time.sleep(pause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbd810",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_rows = []\n",
    "for _, r in tqdm(results_df.head(50).iterrows(), total=50):\n",
    "    orig = r['original_prompt']\n",
    "    tag  = r['model']\n",
    "    cand = df_merged.loc[df_merged['original_prompt']==orig, f\"resp_{tag}\"].iloc[0]\n",
    "    rubric_json = evaluate_with_rubric(orig, cand)\n",
    "\n",
    "    if not rubric_json:\n",
    "        continue\n",
    "    flat = {\n",
    "        'prompt': orig,\n",
    "        'model':  tag,\n",
    "        'lr_score': r['score'],\n",
    "        'rubric_score': rubric_json.get('weighted_score'),\n",
    "        **{k: rubric_json.get(k) for k in [\n",
    "            'task_category','relevance','accuracy','completeness',\n",
    "            'clarity','creativity','conciseness',\n",
    "            'technical_correctness','actionability'\n",
    "        ]}\n",
    "    }\n",
    "    rubric_rows.append(flat)\n",
    "\n",
    "rubric_df = pd.DataFrame(rubric_rows)\n",
    "print(\"Sample means by model:\")\n",
    "print(rubric_df.groupby('model')['rubric_score'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2af54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../data/PromptEval/newRubric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75834ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sample_df = df_merged.sample(n=50, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def score_response(prompt: str, response: str) -> float:\n",
    "    \"\"\"\n",
    "    Uses the enhanced Gemini rubric to score a model's response.\n",
    "    Falls back to 0.0 if missing or malformed.\n",
    "    \"\"\"\n",
    "    rubric = evaluate_with_rubric(prompt, response)\n",
    "    return float(rubric.get(\"weighted_score\", 0.0))\n",
    "\n",
    "variant_cols = [c for c in sample_df.columns if c.startswith(\"resp_\")]\n",
    "response_scores = []\n",
    "\n",
    "for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    orig = row[\"original_prompt\"]\n",
    "    for col in variant_cols:\n",
    "        cand = row[col]\n",
    "        if pd.isna(cand):\n",
    "            continue\n",
    "        # optional: drop if it drifts too much in meaning\n",
    "        if not semantic_ok(orig, cand):\n",
    "            continue\n",
    "        response_scores.append({\n",
    "            \"original_prompt\": orig,\n",
    "            \"model\": col.replace(\"resp_\", \"\"),\n",
    "            \"score\": score_response(orig, cand)\n",
    "        })\n",
    "\n",
    "responses_sample_df = pd.DataFrame(response_scores)\n",
    "print(\"Scored sample of\", len(responses_sample_df), \"response‑variant pairs\")\n",
    "responses_sample_df.head()\n",
    "\n",
    "output_path = \"../data/responses_sample_scores.csv\"\n",
    "responses_sample_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved sample scores → {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef552d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ── 1) Compute mean by model ──\n",
    "reg_means    = results_df         .groupby('model')['score'].mean()\n",
    "full_rub_means = rubric_df       .groupby('model')['rubric_score'].mean()\n",
    "sample_means = responses_sample_df.groupby('model')['score'].mean()\n",
    "\n",
    "# ── 2) Normalize all to [0,1] ──\n",
    "# Regression: min→0, max→1\n",
    "rmin, rmax    = reg_means.min(), reg_means.max()\n",
    "reg_norm      = (reg_means - rmin) / (rmax - rmin)\n",
    "# Rubrics: (value−1)/4 maps [1–5]→[0–1]\n",
    "full_norm     = (full_rub_means - 1) / 4\n",
    "sample_norm   = (sample_means   - 1) / 4\n",
    "\n",
    "# ── 3) Build a summary table ──\n",
    "models = ['none','v1','v2','base']\n",
    "df_summary = pd.DataFrame({\n",
    "    'Regression':    reg_norm.reindex(models),\n",
    "    'Full Rubric':   full_norm.reindex(models),\n",
    "    'Sample Rubric': sample_norm.reindex(models),\n",
    "}, index=models)\n",
    "df_summary.index.name = 'Model'\n",
    "\n",
    "# print the table\n",
    "print(df_summary.to_markdown())\n",
    "\n",
    "# ── 4) Plot grouped bar chart ──\n",
    "x     = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "bars1 = ax.bar(x - width, df_summary['Regression'],    width, label='Regression')\n",
    "bars2 = ax.bar(x,       df_summary['Full Rubric'],   width, label='Full Rubric')\n",
    "bars3 = ax.bar(x + width, df_summary['Sample Rubric'], width, label='Response Rubric')\n",
    "\n",
    "# annotate each bar\n",
    "for bar in list(bars1) + list(bars2) + list(bars3):\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2,\n",
    "            h + 0.02,\n",
    "            f\"{h:.2f}\",\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Mean Normalized Score')\n",
    "ax.set_title('Prompt Quality by Model (Regression vs Full & Responses Rubric)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dc727",
   "metadata": {},
   "source": [
    "## Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d29cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "sample_df = df_merged.sample(n=200, random_state=42).reset_index(drop=True)\n",
    "variant_cols = [c for c in sample_df.columns if c.startswith(\"resp_\")]\n",
    "\n",
    "texts = set(sample_df[\"original_prompt\"])\n",
    "for col in variant_cols:\n",
    "    texts.update(sample_df[col].dropna())\n",
    "texts = list(texts)\n",
    "print(f\"Embedding {len(texts)} unique texts via single‑item calls…\")\n",
    "\n",
    "def embed_text(text: str):\n",
    "    try:\n",
    "        resp = genai.embed_content(\n",
    "            model=\"models/embedding-001\",\n",
    "            content=text,\n",
    "            task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "        )\n",
    "        # for single input, API returns 'embedding'\n",
    "        if \"embedding\" in resp:\n",
    "            return np.array(resp[\"embedding\"])\n",
    "        if \"embeddings\" in resp:\n",
    "            blob = resp[\"embeddings\"][0]\n",
    "            return np.array(blob.values if hasattr(blob, \"values\") else blob[\"values\"])\n",
    "        print(\"⚠️ No embedding key for:\", text[:40])\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(\"🔗 Embed error:\", e)\n",
    "        return None\n",
    "    finally:\n",
    "        sleep(0.1)  # throttle\n",
    "\n",
    "emb_dict = {}\n",
    "for txt in texts:\n",
    "    vec = embed_text(txt)\n",
    "    if vec is not None:\n",
    "        emb_dict[txt] = vec\n",
    "print(\"Got embeddings for\", len(emb_dict), \"texts\")\n",
    "\n",
    "# Calculates how similar the two input vectors are (cosine similarity)\n",
    "def cosine(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))\n",
    "\n",
    "for col in variant_cols:\n",
    "    sim_col = f\"sim_{col}\"\n",
    "    sample_df[sim_col] = [\n",
    "        cosine(emb_dict[o], emb_dict[c])\n",
    "        if o in emb_dict and c in emb_dict else np.nan\n",
    "        for o, c in zip(sample_df[\"original_prompt\"], sample_df[col])\n",
    "    ]\n",
    "\n",
    "sim_cols = [f\"sim_{c}\" for c in variant_cols]\n",
    "print(sample_df[sim_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from google.api_core.retry import Retry\n",
    "\n",
    "def is_retryable(exc):\n",
    "    return hasattr(exc, \"code\") and exc.code in {429, 503}\n",
    "\n",
    "retry_policy = Retry(\n",
    "    predicate=is_retryable,\n",
    "    initial=1.0,    # 1s back‑off\n",
    "    maximum=60.0,   # up to 60s\n",
    "    multiplier=2.0, # exponential back‑off\n",
    "    deadline=300.0  # give up after 5 min\n",
    ")\n",
    "\n",
    "\n",
    "model.generate_content = retry_policy(model.generate_content)\n",
    "\n",
    "\n",
    "MINUTE  = 60\n",
    "ALLOWED = 15\n",
    "delay   = MINUTE / ALLOWED  # → 4.0 seconds\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (variant_cols[i], variant_cols[j])\n",
    "    for i in range(len(variant_cols))\n",
    "    for j in range(i + 1, len(variant_cols))\n",
    "]\n",
    "\n",
    "\n",
    "sampled_df = df_merged.sample(n=200, random_state=42)\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(sampled_df.iterrows(), total=len(sampled_df), desc=\"Pairwise eval\"):\n",
    "    prompt = row['original_prompt']\n",
    "    for col_a, col_b in pairs:\n",
    "        resp_a, resp_b = row[col_a], row[col_b]\n",
    "        prompt_text = (\n",
    "            \"You are an AI evaluator that compares responses.\\n\\n\"\n",
    "            f\"Prompt: {prompt}\\n\\n\"\n",
    "            f\"Response A: {resp_a}\\n\\n\"\n",
    "            f\"Response B: {resp_b}\\n\\n\"\n",
    "            \"Which response is better? Reply with 'A' or 'B'.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = model.generate_content(prompt_text)\n",
    "            choice = response.text.strip().upper()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating comparison: {e}\")\n",
    "            choice = \"ERROR\"\n",
    "\n",
    "        results.append({\n",
    "            'original_prompt': prompt,\n",
    "            'variant_a': col_a,\n",
    "            'variant_b': col_b,\n",
    "            'choice': choice,\n",
    "        })\n",
    "\n",
    "        # throttle to ≤15 requests/minute\n",
    "        sleep(delay)\n",
    "\n",
    "\n",
    "pairwise_results = pd.DataFrame(results)\n",
    "pairwise_results.to_csv('pairwise_results.csv', index=False)\n",
    "\n",
    "\n",
    "pairwise_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process pairwise results\n",
    "pairwise_df = pd.read_csv('pairwise_results.csv')\n",
    "\n",
    "# Count wins for each variant\n",
    "variant_wins = {}\n",
    "for _, row in pairwise_df.iterrows():\n",
    "    if row['choice'] == 'A':\n",
    "        winner = row['variant_a']\n",
    "    elif row['choice'] == 'B':\n",
    "        winner = row['variant_b']\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    variant_wins[winner] = variant_wins.get(winner, 0) + 1\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "variants = list(variant_wins.keys())\n",
    "wins = list(variant_wins.values())\n",
    "\n",
    "plt.bar(variants, wins)\n",
    "plt.title('Number of Wins by Model Variant')\n",
    "plt.xlabel('Model Variant')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
