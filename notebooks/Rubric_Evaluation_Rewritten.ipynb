{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "3b01205f", "cell_type": "markdown", "source": "# Rubric Evaluation Notebook\n\nThis notebook uses rubric-based evaluation powered by embeddings to assess text responses. It defines criteria for scoring, computes semantic similarity using embeddings, and returns category-based and overall evaluations. The system processes data in batches and outputs both granular and summary insights.\n    ", "metadata": {}}, {"id": "98641963", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "import numpy as np\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom chromadb import Documents, Embeddings\nimport google.generativeai as genai\nfrom google.api_core import exceptions\nfrom chromadb.utils.embedding_functions import EmbeddingFunction\n    ", "outputs": []}, {"id": "8dd0cb94", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "# Gemini embedding function for documents and queries\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    document_mode = True  # Set to False for query embedding\n\n    def is_retriable(e):\n        return isinstance(e, exceptions.ServiceUnavailable) or \\\n               isinstance(e, exceptions.ResourceExhausted) or \\\n               (hasattr(e, 'code') and e.code in {429, 503})\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        embedding_task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n        if isinstance(input, str):\n            input = [input]\n        return [genai.embed_content(model=\"models/embedding-001\", content=text, task_type=embedding_task)['embedding']\n                for text in input]\n    ", "outputs": []}, {"id": "22ecf93c", "cell_type": "markdown", "source": "## Simple Rubric Grading (4-level)\n\nThis system evaluates prompts based on similarity to a fixed set of rubric descriptors representing 4 grade levels: Excellent, Good, Average, and Poor.\n    ", "metadata": {}}, {"id": "3e5cbefe", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "class RubricGrader:\n    def __init__(self):\n        self.embedding_function = GeminiEmbeddingFunction()\n        self.rubric_prompts = {\n            \"Excellent\": \"Well-organized, thorough, in-depth analysis with strong supporting evidence.\",\n            \"Good\": \"Covers most points with clarity, some minor lack of depth.\",\n            \"Average\": \"Basic response with limited analysis or structure.\",\n            \"Poor\": \"Incomplete, incoherent, or missing key points.\"\n        }\n        self.rubric_embeddings = self._compute_rubric_embeddings()\n\n    def _compute_rubric_embeddings(self):\n        descriptions = list(self.rubric_prompts.values())\n        embeddings = self.embedding_function(descriptions)\n        return {grade: emb for grade, emb in zip(self.rubric_prompts.keys(), embeddings)}\n\n    def grade_submission(self, submission_prompt: str):\n        embedding = self.embedding_function([submission_prompt])[0]\n        best_grade, best_sim = max(\n            ((grade, cosine_similarity([embedding], [rub_emb])[0][0])\n             for grade, rub_emb in self.rubric_embeddings.items()),\n            key=lambda x: x[1]\n        )\n        return best_grade, best_sim\n    ", "outputs": []}, {"id": "191ec9ff", "cell_type": "markdown", "source": "## Fine-grained Evaluation with Rubric Categories\n\nEach response is evaluated on four dimensions: Relevance, Accuracy, Completeness, Clarity. Each dimension is scored from 1 to 5 based on similarity to descriptive anchors.\n    ", "metadata": {}}, {"id": "42e2f1a3", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "class ResponseEvaluator:\n    def __init__(self):\n        self.embedding_function = GeminiEmbeddingFunction()\n        self.category_rubrics = {\n            'Relevance': {\n                5: \"Fully addresses all prompt aspects with perfect alignment.\",\n                4: \"Covers main points with strong relevance.\",\n                3: \"Some relevant content with partial alignment.\",\n                2: \"Significant deviation from prompt.\",\n                1: \"Fails to address prompt.\"\n            },\n            'Accuracy': {\n                5: \"Completely factual and verifiable.\",\n                4: \"Mostly accurate with minor errors.\",\n                3: \"Mixed accuracy and issues.\",\n                2: \"Contains multiple inaccuracies.\",\n                1: \"Largely incorrect.\"\n            },\n            'Completeness': {\n                5: \"Covers all elements thoroughly with insight.\",\n                4: \"Thoroughly covers main points.\",\n                3: \"Covers most elements, some gaps.\",\n                2: \"Misses several elements.\",\n                1: \"Significantly incomplete.\"\n            },\n            'Clarity': {\n                5: \"Exceptionally clear and well-structured.\",\n                4: \"Well-organized and understandable.\",\n                3: \"Understandable but some disorganization.\",\n                2: \"Poorly structured or vague.\",\n                1: \"Confusing and unclear.\"\n            }\n        }\n        self.category_embeddings = self._compute_rubric_embeddings()\n\n    def _compute_rubric_embeddings(self):\n        return {\n            cat: {lvl: self.embedding_function([desc])[0] for lvl, desc in levels.items()}\n            for cat, levels in self.category_rubrics.items()\n        }\n\n    def evaluate_response(self, response: str):\n        resp_emb = self.embedding_function([response])[0]\n        scores, similarities = {}, {}\n        for cat, level_embs in self.category_embeddings.items():\n            scored_levels = {lvl: cosine_similarity([resp_emb], [emb])[0][0] for lvl, emb in level_embs.items()}\n            best_lvl = max(scored_levels.items(), key=lambda x: x[1])\n            scores[cat], similarities[cat] = best_lvl\n        return {\n            'scores': scores,\n            'similarities': similarities,\n            'overall_score': np.mean(list(scores.values())),\n            'strongest_category': max(similarities.items(), key=lambda x: x[1])[0],\n            'weakest_category': min(similarities.items(), key=lambda x: x[1])[0]\n        }\n    ", "outputs": []}]}