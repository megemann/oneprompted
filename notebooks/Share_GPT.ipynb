{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load From Hugging Face Datasets\n",
    "- Manually extract each datapoint to allow it to be put into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "# Load using streaming (often bypasses parsing errors)\n",
    "dataset = load_dataset(\"RyokoAI/ShareGPT52K\", split=\"train\", streaming=True)\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through the dataset and collect the examples\n",
    "for example in dataset:\n",
    "    data.append(example)\n",
    "\n",
    "# Let's extract conversations and group them by their source\n",
    "conversations_by_source = []\n",
    "\n",
    "for item in data:\n",
    "    # Check if the item has a 'conversations' attribute\n",
    "    if 'conversations' in item:\n",
    "        # Add the entire conversation as one item\n",
    "        conversations_by_source.append(item['conversations'])\n",
    "\n",
    "# Display information about the extracted conversations\n",
    "print(f\"Total conversations extracted: {len(conversations_by_source)}\")\n",
    "print(\"\\nFirst 3 conversations:\")\n",
    "for i, conversation in enumerate(conversations_by_source[:3]):\n",
    "    print(f\"Conversation {i+1}:\")\n",
    "    for j, message in enumerate(conversation):\n",
    "        role = message.get('from', 'unknown')\n",
    "        content = message.get('value', '')\n",
    "        # Print a truncated version if the message is too long\n",
    "        if len(content) > 100:\n",
    "            print(f\"  {role}: {content[:100]}...\")\n",
    "        else:\n",
    "            print(f\"  {role}: {content}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Create a DataFrame with the conversations\n",
    "conversations_df = pd.DataFrame({'conversation': conversations_by_source})\n",
    "\n",
    "# Display information about the conversations DataFrame\n",
    "print(\"\\nConversations DataFrame:\")\n",
    "print(f\"Shape: {conversations_df.shape}\")\n",
    "print(conversations_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter out non-english conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import langdetect for language detection\n",
    "import langdetect\n",
    "from langdetect import DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for deterministic language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Function to check if a conversation contains non-English messages\n",
    "def conversation_has_non_english(conversation, english_langs=['en']):\n",
    "    \"\"\"\n",
    "    Check if any message in the conversation is non-English.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of message dictionaries\n",
    "        english_langs: List of language codes considered as English\n",
    "        \n",
    "    Returns:\n",
    "        True if any message is detected as non-English, False otherwise\n",
    "    \"\"\"\n",
    "    for message in conversation:\n",
    "        content = message.get('value', '')\n",
    "        # Skip very short messages or invalid text\n",
    "        if not isinstance(content, str) or len(content.strip()) <= 20:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Detect language of the text\n",
    "            detected_lang = langdetect.detect(content)\n",
    "            if detected_lang not in english_langs:\n",
    "                return True\n",
    "        except LangDetectException:\n",
    "            # If detection fails, assume it's valid (English)\n",
    "            continue\n",
    "            \n",
    "    return False\n",
    "\n",
    "# Filter out conversations with non-English content\n",
    "english_conversations = []\n",
    "for conversation in tqdm(conversations_by_source, desc=\"Filtering non-English conversations\"):\n",
    "    if not conversation_has_non_english(conversation):\n",
    "        english_conversations.append(conversation)\n",
    "\n",
    "print(f\"Original number of conversations: {len(conversations_by_source)}\")\n",
    "print(f\"Number of conversations after filtering non-English: {len(english_conversations)}\")\n",
    "\n",
    "# Update the DataFrame with only English conversations\n",
    "english_conversations_df = pd.DataFrame({'conversation': english_conversations})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract human prompts from LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract human prompts from conversations\n",
    "print(\"\\nExtracting human prompts from conversations...\")\n",
    "human_prompts = []\n",
    "\n",
    "for i, conversation in enumerate(english_conversations):\n",
    "    conversation_id = i  # Assign a unique ID to each conversation\n",
    "    for message in conversation:\n",
    "        # Check if the message is from a human (typically 'human' or 'user')\n",
    "        if message.get('from', '').lower() in ['human', 'user']:\n",
    "            # Get the content of the message\n",
    "            content = message.get('value', '')\n",
    "            if content:  # Only add non-empty prompts\n",
    "                human_prompts.append({\n",
    "                    'conversation_id': conversation_id,\n",
    "                    'prompt': content\n",
    "                })\n",
    "\n",
    "# Create a DataFrame with human prompts and conversation IDs\n",
    "human_prompts_df = pd.DataFrame(human_prompts)\n",
    "\n",
    "#print(\"Filtering to keep only the first message from each unique conversation...\")\n",
    "first_messages_df = human_prompts_df.drop_duplicates(subset=['conversation_id'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 & 5. Filter out short messages and sample 1000 examples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of 1000 prompts with more than 3 words and save them to a new CSV file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Filter prompts with more than 3 words\n",
    "filtered_prompts = first_messages_df[first_messages_df['word_count'] > 3]\n",
    "\n",
    "# Take a random sample of 1000 prompts (or all if less than 1000)\n",
    "sample_size = min(1000, len(filtered_prompts))\n",
    "filtered_prompts.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Save the sampled prompts to a new CSV file\n",
    "output_file = '../data/ShareGPT/testing_prompts_cleaned.csv'\n",
    "sampled_prompts.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved {sample_size} randomly sampled prompts (>3 words) to {output_file}\")\n",
    "print(f\"Sample statistics:\")\n",
    "print(f\"- Mean word count: {sampled_prompts['word_count'].mean():.2f} words\")\n",
    "print(f\"- Median word count: {sampled_prompts['word_count'].median():.2f} words\")\n",
    "print(f\"- Min word count: {sampled_prompts['word_count'].min()} words\")\n",
    "print(f\"- Max word count: {sampled_prompts['word_count'].max()} words\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
