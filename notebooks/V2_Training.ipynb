{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we talk about seperating prompts with the widgets\n",
    "for context and such\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from google.api_core import retry\n",
    "import os\n",
    "# Import environment variables from env.json\n",
    "import json\n",
    "\n",
    "# Load environment variables from env.json\n",
    "with open('../../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n",
    "# Set environment variables from the loaded file\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = env_vars[\"google_cloud_project\"]\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = env_vars[\"google_cloud_location\"]\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = env_vars[\"google_genai_use_vertexai\"]\n",
    "# Set the fine-tuned model ID as an environment variable\n",
    "os.environ[\"GOOGLE_API_KEY\"] = env_vars[\"google_api_keys\"][1]\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Use a specific model\n",
    "model = genai.GenerativeModel('models/gemini-2.5-pro-preview-03-25')  # base untuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(instruction, context):\n",
    "    if context == \"\" or context == None:\n",
    "        context = \"No context provided\" \n",
    "    prompt = f'''\n",
    "    You are a prompt engineering expert.\n",
    "\n",
    "    Your task is to rewrite the instruction below using advanced prompt engineering techniques. If context is provided, use it as *background knowledge* to better understand the task — but do not include it in the final output.\n",
    "\n",
    "    Guidelines:\n",
    "    - Enhance the instruction to be clearer, more specific, and more effective\n",
    "    - Use any prompting technique that best fits\n",
    "    - Ground your rewrite in the provided context, if applicable\n",
    "    - Do NOT copy or reference the context in your rewritten instruction\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Original Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    Output ONLY the improved instruction without any additional text, titling, explanations, or acknowledgment.\n",
    "    '''\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\pandas\\__init__.py:27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dependency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     29\u001b[39m         _missing_dependencies.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\numpy\\__init__.py:194\u001b[39m\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m ta\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scimath \u001b[38;5;28;01mas\u001b[39;00m emath\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_histograms_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    197\u001b[39m     histogram, histogram_bin_edges, histogramdd\n\u001b[32m    198\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\numpy\\lib\\__init__.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m introspect\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mixins\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m npyio\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scimath\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stride_tricks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_npyio_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     \u001b[34m__doc__\u001b[39m, DataSource, NpzFile\n\u001b[32m      3\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_datasource\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataSource\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packbits, unpackbits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1091\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1190\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load the validation data\n",
    "validation_data_path = \"../../data/Prompt_Training_2.0/seperated_validation_data.csv\"\n",
    "output_path = \"../../data/Prompt_Training_2.0/labeled_validation_data.csv\"\n",
    "\n",
    "# Load the validation data\n",
    "testing_data_path = \"../../data/Prompt_Training_2.0/seperated_test_data.csv\"\n",
    "output_path_2 = \"../../data/Prompt_Training_2.0/labeled_test_data.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(validation_data_path):\n",
    "    print(f\"Error: File {validation_data_path} not found.\")\n",
    "else:\n",
    "    # Load the data\n",
    "    df = pd.read_csv(validation_data_path)\n",
    "    test_df = pd.read_csv(testing_data_path)\n",
    "    print(f\"Loaded {len(df)} rows from validation data.\")\n",
    "    \n",
    "    # Create a new column for improved prompts\n",
    "    df['improved_prompt'] = None\n",
    "    test_df['improved_prompt'] = None\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating improved prompts\"):\n",
    "        instruction = row['instruction']\n",
    "        context = row['context']\n",
    "        \n",
    "        try:\n",
    "            # Generate improved prompt\n",
    "            improved_prompt = generate_response(instruction, context)\n",
    "            df.at[idx, 'improved_prompt'] = improved_prompt\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            df.at[idx, 'improved_prompt'] = f\"Error: {str(e)}\"\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating improved prompts\"):\n",
    "        instruction = row['instruction']\n",
    "        context = row['context']\n",
    "        \n",
    "        try:\n",
    "            # Generate improved prompt\n",
    "            improved_prompt = generate_response(instruction, context)\n",
    "            test_df.at[idx, 'improved_prompt'] = improved_prompt\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            test_df.at[idx, 'improved_prompt'] = f\"Error: {str(e)}\"\n",
    "\n",
    "    # Save the results\n",
    "    df.to_csv(output_path, index=False)\n",
    "    test_df.to_csv(output_path_2, index=False)\n",
    "    print(f\"Saved results to {output_path}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add post processing steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the final datasets \n",
    "validation_df = pd.read_csv('../data/Prompt_Training_2.0/labeled_validation_final.csv')\n",
    "train_df = pd.read_csv('../data/Prompt_Training_2.0/labeled_train_final.csv')\n",
    "\n",
    "# Build the prompt template \n",
    "prompt_template = \"\"\"You are a prompt engineering expert.\n",
    "\n",
    "Your task is to rewrite the instruction below using advanced prompt engineering techniques. If context is provided, use it as *background knowledge* to better understand the task — but do not include it in the final output.\n",
    "\n",
    "Guidelines:\n",
    "- Enhance the instruction to be clearer, more specific, and more effective\n",
    "- Use any prompting technique that best fits\n",
    "- Ground your rewrite in the provided context, if applicable\n",
    "- Do NOT copy or reference the context in your rewritten instruction\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Original Instruction:\n",
    "{instruction}\n",
    "\n",
    "Output ONLY the improved instruction without any additional text, titling, explanations, or acknowledgment.\"\"\"\n",
    "\n",
    "# Function to create JSONL entries\n",
    "def create_jsonl_entries(df, include_system_prompt=False):\n",
    "    entries = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Format the prompt with the template\n",
    "        context = row['context'] if pd.notna(row['context']) else \"\"\n",
    "        instruction = row['instruction'] if pd.notna(row['instruction']) else \"\"\n",
    "        improved = row['improved_instruction'] if pd.notna(row['improved_instruction']) else \"\"\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(context=context, instruction=instruction)\n",
    "        \n",
    "        # Create the entry based on whether to include system prompt\n",
    "        if include_system_prompt:\n",
    "            entry = {\n",
    "                \"systemInstruction\": {\n",
    "                    \"role\": \"system\", \n",
    "                    \"parts\": [{\"text\": \"You are a prompt engineering expert that transforms prompts into more effective versions.\"}]\n",
    "                }, \n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"parts\": [{\"text\": formatted_prompt}]\n",
    "                    }, \n",
    "                    {\n",
    "                        \"role\": \"model\", \n",
    "                        \"parts\": [{\"text\": improved}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            entry = {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"parts\": [{\"text\": formatted_prompt}]\n",
    "                    }, \n",
    "                    {\n",
    "                        \"role\": \"model\", \n",
    "                        \"parts\": [{\"text\": improved}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        \n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "# Create JSONL entries for training and validation\n",
    "train_entries = create_jsonl_entries(train_df, include_system_prompt=False)\n",
    "validation_entries = create_jsonl_entries(validation_df, include_system_prompt=False)\n",
    "\n",
    "# Save to JSONL files\n",
    "train_jsonl_path = '../data/Prompt_Training_2.0/vertex_ai_fine_tuning_train.jsonl'\n",
    "validation_jsonl_path = '../data/Prompt_Training_2.0/vertex_ai_fine_tuning_validation.jsonl'\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(os.path.dirname(train_jsonl_path), exist_ok=True)\n",
    "\n",
    "# Write training data\n",
    "with open(train_jsonl_path, 'w') as f:\n",
    "    for entry in train_entries:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Write validation data\n",
    "with open(validation_jsonl_path, 'w') as f:\n",
    "    for entry in validation_entries:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(f\"Created training JSONL file at {train_jsonl_path}\")\n",
    "print(f\"Total training examples: {len(train_entries)}\")\n",
    "\n",
    "print(f\"Created validation JSONL file at {validation_jsonl_path}\")\n",
    "print(f\"Total validation examples: {len(validation_entries)}\")\n",
    "\n",
    "# Display a sample entry\n",
    "print(\"\\nSample entry:\")\n",
    "print(json.dumps(train_entries[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n",
    "# Set environment variables from the loaded file\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = env_vars[\"google_cloud_project\"]\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = env_vars[\"google_cloud_location\"]\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = env_vars[\"google_genai_use_vertexai\"]\n",
    "# Set the fine-tuned model ID as an environment variable\n",
    "os.environ[\"FINE_TUNED_MODEL_ID\"] = env_vars[\"fine_tuned_v2_model_id\"]\n",
    "\n",
    "\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "# Initialize the GenAI client for Vertex AI\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "\n",
    "# Define a test prompt\n",
    "instruction = \"Write a function to calculate the Fibonacci sequence in Python\"\n",
    "context = \"The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.\"\n",
    "\n",
    "test_prompt = prompt_template.format(context=context, instruction=instruction)\n",
    "\n",
    "# Call the fine-tuned model\n",
    "response = client.models.generate_content(\n",
    "    model=os.environ[\"FINE_TUNED_MODEL_ID\"],\n",
    "    contents=test_prompt,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(\"Original prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\nImproved prompt from fine-tuned model:\")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
