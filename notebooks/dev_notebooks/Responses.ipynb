{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from google.api_core import retry\n",
    "import os\n",
    "# Import environment variables from env.json\n",
    "import json\n",
    "\n",
    "# Load environment variables from env.json\n",
    "with open('../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n",
    "# Set environment variables from the loaded file\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = env_vars[\"google_cloud_project\"]\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = env_vars[\"google_cloud_location\"]\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = env_vars[\"google_genai_use_vertexai\"]\n",
    "# Set the fine-tuned model ID as an environment variable\n",
    "os.environ[\"FINE_TUNED_MODEL_ID\"] = env_vars[\"fine_tuned_v1_model_id\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = env_vars[\"google_api_keys\"][1]\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/veo-2.0-generate-001\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m model = genai.GenerativeModel(\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash-lite-001\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# base untuned model\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generate content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow can I be a better team leader?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    289\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    290\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    146\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:79\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     81\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m   1192\u001b[39m     (\n\u001b[32m   1193\u001b[39m         state,\n\u001b[32m   1194\u001b[39m         call,\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\austi\\oneprompted\\.venv\\Lib\\site-packages\\grpc\\_channel.py:1162\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1145\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1146\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1147\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1160\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1161\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1163\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:78\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:42\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# List available models\n",
    "print(\"Available models:\")\n",
    "for m in genai.list_models():\n",
    "    print(m.name)\n",
    "\n",
    "# Use a specific model\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-lite-001')  # base untuned model\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content(\"How can I be a better team leader?\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're talking to a 7-year-old. Explain what Artificial Intelligence (AI) is, using simple words and examples they can easily understand. Focus on how AI helps computers do smart things, like playing games or recognizing pictures. Break down the explanation into three short paragraphs. In the first paragraph, define AI in a general way. In the second, give an example of AI in action. In the third, explain a simple limitation of AI. End your response with a single sentence summarizing what AI can do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The genai module doesn't have a Client attribute, so we'll use the GenerativeModel directly\n",
    "# No need to create a client instance\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = f'''You are a prompt engineering expert that transforms simple \n",
    "    prompts into more effective versions. Analyze the input prompt and create\n",
    "      an improved version that includes specific details, context, desired output format, \n",
    "      and any relevant constraints. Make the prompt clear, specific, and designed to \n",
    "      generate high-quality responses.\n",
    "      \n",
    "      Input Prompt: {prompt}\n",
    "\n",
    "      Respond ONLY with the text of the improved prompt, without any explanations, \n",
    "      introductions, or additional commentary.\n",
    "      '''\n",
    "\n",
    "    response = model.generate_content(contents=inputs)\n",
    "    return response.text\n",
    "\n",
    "print(generate_response('Explain AI to me like im a little kid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 962 rows\n",
      "\n",
      "Dataset columns:\n",
      "- original_prompt\n",
      "- context\n",
      "- instruction\n",
      "- has_context\n",
      "- conversation_id\n",
      "\n",
      "First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>instruction</th>\n",
       "      <th>has_context</th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>False</td>\n",
       "      <td>93453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>The content:\\n\\nConsumers want more choices, b...</td>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>65263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>False</td>\n",
       "      <td>99000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>One of those properties will be center point, ...</td>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>True</td>\n",
       "      <td>96296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "      <td>False</td>\n",
       "      <td>38806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "3  make a javascript class \"GraphicLayer\" which i...   \n",
       "4  Please outline the steps to build an automated...   \n",
       "\n",
       "                                             context  \\\n",
       "0                                                NaN   \n",
       "1  The content:\\n\\nConsumers want more choices, b...   \n",
       "2                                                NaN   \n",
       "3  One of those properties will be center point, ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         instruction  has_context  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights        False   \n",
       "1  We have the following blog content... what is ...         True   \n",
       "2  how o sort element using merge sort technique ...        False   \n",
       "3  make a javascript class \"GraphicLayer\" which i...         True   \n",
       "4  Please outline the steps to build an automated...        False   \n",
       "\n",
       "   conversation_id  \n",
       "0            93453  \n",
       "1            65263  \n",
       "2            99000  \n",
       "3            96296  \n",
       "4            38806  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ShareGPT dataset from CSV\n",
    "try:\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = \"../data/Prompt_Training_2.0/seperated_test_data.csv\"\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df_sharegpt = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset loaded successfully with {len(df_sharegpt)} rows\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in df_sharegpt.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Display the first few rows of the dataset\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    display(df_sharegpt.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '../data/Prompt_Training_2.0/seperated_test_data.csv' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {str(e)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating improved prompts for each original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 962/962 [37:52<00:00,  2.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 962 improved prompts\n",
      "Final results saved to ../data/BaseModelResponses/improved_prompts.csv\n",
      "\n",
      "Sample of results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>base_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>Create a list of five (5) unique one-pot veget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>Analyze the provided blog content about choosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>Write a Java program that implements the Merge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "\n",
       "                                       base_response  \n",
       "0  Create a list of five (5) unique one-pot veget...  \n",
       "1  Analyze the provided blog content about choosi...  \n",
       "2  Write a Java program that implements the Merge...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the generate_response function to each original_prompt in the dataframe\n",
    "# This will create a new column 'base_response' with the improved prompts\n",
    "print(\"Generating improved prompts for each original prompt...\")\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to safely apply generate_response\n",
    "def safe_generate_response(prompt):\n",
    "    try:\n",
    "        return generate_response(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {str(e)[:100]}...\")\n",
    "        return \"Error generating response\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/BaseModelResponses\", exist_ok=True)\n",
    "\n",
    "# Initialize counter for saving\n",
    "counter = [0]\n",
    "total_rows = len(df_sharegpt)\n",
    "\n",
    "# Define a function to process each row and save every 100 rows\n",
    "def process_and_save(prompt):\n",
    "    response = safe_generate_response(prompt)\n",
    "    return response\n",
    "\n",
    "# Apply the function to each row in the dataframe with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Processing prompts\")\n",
    "df_sharegpt['base_response'] = df_sharegpt['original_prompt'].progress_apply(process_and_save)\n",
    "\n",
    "# Save the final complete dataset\n",
    "df_sharegpt.to_csv(\"../data/BaseModelResponses/improved_prompts.csv\", index=False)\n",
    "print(f\"Completed generating {total_rows} improved prompts\")\n",
    "print(f\"Final results saved to ../data/BaseModelResponses/improved_prompts.csv\")\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "display(df_sharegpt[['original_prompt', 'base_response']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a seasoned leadership consultant. Provide a comprehensive guide on how to improve team leadership skills. Break down the guide into the following sections:\n",
      "\n",
      "1.  **Understanding the Fundamentals:** Define team leadership, discuss its importance, and outline the core responsibilities of a team leader.\n",
      "2.  **Building Effective Communication:** Detail strategies for clear, concise, and empathetic communication within a team. Include tips on active listening, providing constructive feedback, and handling difficult conversations.\n",
      "3.  **Motivating and Engaging Team Members:** Explain different motivational techniques, such as recognition, rewards, and opportunities for growth. Discuss how to create a positive and engaging work environment.\n",
      "4.  **Delegation and Empowerment:** Describe how to delegate tasks effectively and empower team members to take ownership and responsibility. Include strategies for monitoring progress and providing support.\n",
      "5.  **Conflict Resolution:** Outline steps for resolving conflicts within a team in a constructive and productive manner. Discuss techniques for mediation and compromise.\n",
      "\n",
      "For each section, provide actionable advice and examples. Use CONTEXTUAL_PROMPTING to tailor the advice to a hypothetical team of software engineers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "\n",
    "\n",
    "# Access the model ID from environment variables\n",
    "fine_tuned_model_id = os.environ[\"FINE_TUNED_MODEL_ID\"]\n",
    "\n",
    "\n",
    "# Initialize the GenAI client for Vertex AI\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"How can I be a better team leader?\"\n",
    "\n",
    "# Call your fine-tuned model\n",
    "response = client.models.generate_content(\n",
    "    model=fine_tuned_model_id,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works with no context, but you can do with context too\n",
    "def generate_response(prompt):\n",
    "    inputs = f'''You are a prompt engineering expert that transforms simple \n",
    "    prompts into more effective versions. Analyze the input prompt and create\n",
    "      an improved version that includes specific details, context, desired output format, \n",
    "      and any relevant constraints. Make the prompt clear, specific, and designed to \n",
    "      generate high-quality responses.\n",
    "      \n",
    "      Input Prompt: {prompt}\n",
    "\n",
    "      Respond ONLY with the text of the improved prompt, without any explanations, \n",
    "      introductions, or additional commentary.\n",
    "      '''\n",
    "\n",
    "    return client.models.generate_content(\n",
    "        model=fine_tuned_model_id,\n",
    "        contents=inputs).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 962 rows\n",
      "\n",
      "Dataset columns:\n",
      "- original_prompt\n",
      "- context\n",
      "- instruction\n",
      "- has_context\n",
      "- conversation_id\n",
      "\n",
      "First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>instruction</th>\n",
       "      <th>has_context</th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>False</td>\n",
       "      <td>93453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>The content:\\n\\nConsumers want more choices, b...</td>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>65263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>False</td>\n",
       "      <td>99000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>One of those properties will be center point, ...</td>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>True</td>\n",
       "      <td>96296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "      <td>False</td>\n",
       "      <td>38806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "3  make a javascript class \"GraphicLayer\" which i...   \n",
       "4  Please outline the steps to build an automated...   \n",
       "\n",
       "                                             context  \\\n",
       "0                                                NaN   \n",
       "1  The content:\\n\\nConsumers want more choices, b...   \n",
       "2                                                NaN   \n",
       "3  One of those properties will be center point, ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         instruction  has_context  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights        False   \n",
       "1  We have the following blog content... what is ...         True   \n",
       "2  how o sort element using merge sort technique ...        False   \n",
       "3  make a javascript class \"GraphicLayer\" which i...         True   \n",
       "4  Please outline the steps to build an automated...        False   \n",
       "\n",
       "   conversation_id  \n",
       "0            93453  \n",
       "1            65263  \n",
       "2            99000  \n",
       "3            96296  \n",
       "4            38806  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ShareGPT dataset from CSV\n",
    "try:\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = \"../data/Prompt_Training_2.0/seperated_test_data.csv\"\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df_sharegpt = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset loaded successfully with {len(df_sharegpt)} rows\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in df_sharegpt.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Display the first few rows of the dataset\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    display(df_sharegpt.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '../data/Prompt_Training_2.0/seperated_test_data.csv' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating improved prompts for each original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 962/962 [26:43<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 962 improved prompts\n",
      "Final results saved to ../data/FineTunedResponses/all_responses.csv\n",
      "\n",
      "Sample of results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>fine_tuned_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>I am looking for one-pot vegetarian pasta reci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>Here's an example of how to analyze blog conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>I need a Java function implementation for the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "\n",
       "                                 fine_tuned_response  \n",
       "0  I am looking for one-pot vegetarian pasta reci...  \n",
       "1  Here's an example of how to analyze blog conte...  \n",
       "2  I need a Java function implementation for the ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the generate_response function to each original_prompt in the dataframe\n",
    "# This will create a new column 'base_response' with the improved prompts\n",
    "print(\"Generating improved prompts for each original prompt...\")\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to safely apply generate_response\n",
    "def safe_generate_response(prompt):\n",
    "    try:\n",
    "        return generate_response(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {str(e)[:100]}...\")\n",
    "        return \"Error generating response\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/FineTunedResponses\", exist_ok=True)\n",
    "\n",
    "# Initialize counter for saving\n",
    "counter = [0]\n",
    "total_rows = len(df_sharegpt)\n",
    "\n",
    "# Define a function to process each row and save every 100 rows\n",
    "def process_and_save(prompt):\n",
    "    response = safe_generate_response(prompt)\n",
    "    counter[0] += 1\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Apply the function to each row in the dataframe with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Processing prompts\")\n",
    "df_sharegpt['fine_tuned_response'] = df_sharegpt['original_prompt'].progress_apply(process_and_save)\n",
    "\n",
    "# Save the final complete dataset\n",
    "df_sharegpt.to_csv(\"../data/FineTunedResponses/all_responses.csv\", index=False)\n",
    "print(f\"Completed generating {total_rows} improved prompts\")\n",
    "print(f\"Final results saved to ../data/FineTunedResponses/all_responses.csv\")\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "display(df_sharegpt[['original_prompt', 'fine_tuned_response']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-pot vegetarian pasta recipes for busy nights\n",
      "Here are a few examples of well-crafted one-pot pasta recipes:\n",
      "\n",
      "Example 1: One-Pot Creamy Tomato Pasta\n",
      "Ingredients: 1 pound pasta, 1 can diced tomatoes, 1 cup vegetable broth, 1/2 cup heavy cream, 1/4 cup chopped basil, salt, pepper\n",
      "Instructions: In a large pot, combine pasta, tomatoes, and broth. Bring to a boil and cook until pasta is al dente. Stir in heavy cream, basil, salt, and pepper. Simmer for a few minutes to thicken.\n",
      "\n",
      "Example 2: One-Pot Lemon Garlic Pasta\n",
      "Ingredients: 1 pound pasta, 1/2 cup olive oil, 4 cloves garlic, minced, 1 lemon, zested and juiced, 1 cup vegetable broth, salt, pepper\n",
      "Instructions: Heat olive oil in a large pot. Sauté garlic until fragrant. Add pasta, lemon zest, and juice, and broth. Bring to a boil and cook until pasta is al dente. Season with salt and pepper.\n",
      "\n",
      "Now, create a one-pot pasta recipe with the following ingredients:\n",
      "\n",
      "Ingredients: 1 pound pasta, 1 jar pesto, 1 can cannellini beans, 1 cup vegetable broth, 1/4 cup Parmesan cheese, salt, pepper\n",
      "Instructions:\n"
     ]
    }
   ],
   "source": [
    "print(df_sharegpt['original_prompt'].iloc[0])\n",
    "\n",
    "print(generate_response(df_sharegpt['original_prompt'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prompt:\n",
      "Write a function to calculate the Fibonacci sequence in Python The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.\n",
      "\n",
      "Improved prompt from fine-tuned model:\n",
      "Generate a Python function named `fibonacci(n)` that computes and returns the n-th Fibonacci number. Ensure the function addresses standard edge cases:\n",
      "1.  If `n` is less than 0, return `ValueError: Input must be a non-negative integer.`.\n",
      "2.  If `n` is exactly 0, return 0.\n",
      "3.  If `n` is exactly 1, return 1.\n",
      "4.  If `n` is greater than 1, employ dynamic programming (DP) for efficient calculation. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model\n",
    "os.environ[\"FINE_TUNED_MODEL_ID\"] = env_vars[\"fine_tuned_v2_model_id\"]\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "# Initialize the GenAI client for Vertex AI\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "\n",
    "def generate_response(instruction, context):\n",
    "    prompt_template = f\"\"\"You are a prompt engineering expert.\n",
    "    Your task is to rewrite the instruction below using advanced prompt engineering techniques. If context is provided, use it as *background knowledge* to better understand the task — but do not include it in the final output.\n",
    "\n",
    "    Guidelines:\n",
    "    - Enhance the instruction to be clearer, more specific, and more effective\n",
    "    - Use any prompting technique that best fits\n",
    "    - Ground your rewrite in the provided context, if applicable\n",
    "    - Do NOT copy or reference the context in your rewritten instruction\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Original Instruction:\n",
    "    {instruction}\n",
    "\n",
    "    Output ONLY the improved instruction without any additional text, titling, explanations, or acknowledgment.\"\"\"\n",
    "\n",
    "    return client.models.generate_content(\n",
    "        model=os.environ[\"FINE_TUNED_MODEL_ID\"],\n",
    "        contents=prompt_template\n",
    "    ).text\n",
    "\n",
    "\n",
    "# Define a test prompt\n",
    "instruction = \"Write a function to calculate the Fibonacci sequence in Python\"\n",
    "context = \"The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.\"\n",
    "\n",
    "# Call the fine-tuned model\n",
    "response = generate_response(instruction, context)\n",
    "\n",
    "# Print the response\n",
    "print(\"Original prompt:\")\n",
    "print(instruction + ' ' +context)\n",
    "print(\"\\nImproved prompt from fine-tuned model:\")\n",
    "print(response + ' ' + context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 962 rows\n",
      "\n",
      "Dataset columns:\n",
      "- original_prompt\n",
      "- context\n",
      "- instruction\n",
      "- has_context\n",
      "- conversation_id\n",
      "\n",
      "First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>instruction</th>\n",
       "      <th>has_context</th>\n",
       "      <th>conversation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>False</td>\n",
       "      <td>93453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>The content:\\n\\nConsumers want more choices, b...</td>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>True</td>\n",
       "      <td>65263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>False</td>\n",
       "      <td>99000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>One of those properties will be center point, ...</td>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>True</td>\n",
       "      <td>96296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "      <td>False</td>\n",
       "      <td>38806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "3  make a javascript class \"GraphicLayer\" which i...   \n",
       "4  Please outline the steps to build an automated...   \n",
       "\n",
       "                                             context  \\\n",
       "0                                                NaN   \n",
       "1  The content:\\n\\nConsumers want more choices, b...   \n",
       "2                                                NaN   \n",
       "3  One of those properties will be center point, ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         instruction  has_context  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights        False   \n",
       "1  We have the following blog content... what is ...         True   \n",
       "2  how o sort element using merge sort technique ...        False   \n",
       "3  make a javascript class \"GraphicLayer\" which i...         True   \n",
       "4  Please outline the steps to build an automated...        False   \n",
       "\n",
       "   conversation_id  \n",
       "0            93453  \n",
       "1            65263  \n",
       "2            99000  \n",
       "3            96296  \n",
       "4            38806  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ShareGPT dataset from CSV\n",
    "try:\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = \"../data/Prompt_Training_2.0/seperated_test_data.csv\"\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df_sharegpt = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset loaded successfully with {len(df_sharegpt)} rows\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in df_sharegpt.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Display the first few rows of the dataset\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    display(df_sharegpt.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '../data/Prompt_Training_2.0/seperated_test_data.csv' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating improved prompts for each original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 962/962 [38:55<00:00,  2.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 962 improved prompts\n",
      "Final results saved to ../data/FineTunedV2Responses/improved_prompts.csv\n",
      "\n",
      "Sample of results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fine_tuned_instruction</th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generate a numbered list of 5-7 one-pot vegeta...</td>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyze the provided text [insert or reference...</td>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>The content:\\n\\nConsumers want more choices, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generate a complete and runnable Java program ...</td>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              fine_tuned_instruction  \\\n",
       "0  Generate a numbered list of 5-7 one-pot vegeta...   \n",
       "1  Analyze the provided text [insert or reference...   \n",
       "2  Generate a complete and runnable Java program ...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "\n",
       "                                             context  \n",
       "0                                                NaN  \n",
       "1  The content:\\n\\nConsumers want more choices, b...  \n",
       "2                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the generate_response function to each original_prompt in the dataframe\n",
    "# This will create a new column 'base_response' with the improved prompts\n",
    "print(\"Generating improved prompts for each original prompt...\")\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to safely apply generate_response\n",
    "def safe_generate_response(instruction, context):\n",
    "    try:\n",
    "        return generate_response(instruction, context)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {str(e)[:100]}...\")\n",
    "        return \"Error generating response\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/FineTunedV2Responses\", exist_ok=True)\n",
    "\n",
    "# Initialize counter for saving\n",
    "counter = [0]\n",
    "total_rows = len(df_sharegpt)\n",
    "\n",
    "# Define a function to process each row and save every 100 rows\n",
    "def process_and_save(row):\n",
    "    instruction = row['instruction']\n",
    "    context = row['context']\n",
    "    response = safe_generate_response(instruction, context)\n",
    "    counter[0] += 1\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Apply the function to each row in the dataframe with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Processing prompts\")\n",
    "df_sharegpt['fine_tuned_instruction'] = df_sharegpt.progress_apply(process_and_save, axis=1)\n",
    "\n",
    "# Save the final complete dataset\n",
    "df_sharegpt.to_csv(\"../data/FineTunedV2Responses/improved_prompts.csv\", index=False)\n",
    "print(f\"Completed generating {total_rows} improved prompts\")\n",
    "print(f\"Final results saved to ../data/FineTunedV2Responses/improved_prompts.csv\")\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "display(df_sharegpt[['fine_tuned_instruction', 'instruction', 'context']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n",
      "\n",
      "Processing fine-tuned prompts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 132/962 [16:38<1:58:39,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 132: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/962 [21:28<1:57:37,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 175: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 421/962 [54:39<57:03,  6.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 421: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 448/962 [58:42<1:28:35, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 448: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 495/962 [1:04:43<40:21,  5.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 495: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 554/962 [1:12:22<1:26:46, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 554: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 638/962 [1:23:11<34:11,  6.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 638: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 756/962 [1:38:09<24:47,  7.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 756: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 773/962 [1:40:24<31:30, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 773: Could not create `Blob`, expected `Blob`, `dict` or an `Image` type(`PIL.Image.Image` or `IPython.display.Image`).\n",
      "Got a: <class 'float'>\n",
      "Value: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 962/962 [2:03:37<00:00,  7.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responses saved to ../data/FineTunedResponses/responses.csv\n",
      "\n",
      "Sample responses to fine-tuned prompts:\n",
      "                                   fine_tuned_prompt  \\\n",
      "0  I am looking for one-pot vegetarian pasta reci...   \n",
      "1  Here's an example of how to analyze blog conte...   \n",
      "2  I need a Java function implementation for the ...   \n",
      "\n",
      "                                            response  \n",
      "0  Okay, here are three one-pot vegetarian pasta ...  \n",
      "1  Okay, I need the blog content to analyze! Plea...  \n",
      "2  ```java\\n// MergeSort.java\\npublic class Merge...  \n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "def get_response(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Rename fine_tuned_response to fine_tuned_prompt in the all_prompts.csv file\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/FineTunedResponses/improved_prompts.csv\")\n",
    "\n",
    "# Rename the column\n",
    "prompts_df = prompts_df.rename(columns={'fine_tuned_response': 'fine_tuned_prompt'})\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['response'] = None\n",
    "\n",
    "# Process each prompt and get a response\n",
    "print(\"\\nProcessing fine-tuned prompts:\")\n",
    "for i in tqdm(range(len(prompts_df))):\n",
    "    try:\n",
    "        # Get the response for the fine-tuned prompt\n",
    "        response = get_response(prompts_df.loc[i, 'fine_tuned_prompt'])\n",
    "        prompts_df.loc[i, 'response'] = response\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        prompts_df.loc[i, 'response'] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Wait a bit longer if there's an error\n",
    "        time.sleep(2)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/FineTunedResponses/responses.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/FineTunedResponses/responses.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to fine-tuned prompts:\")\n",
    "print(prompts_df[['fine_tuned_prompt', 'response']].head(3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BaseModelResponses prompts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 531/962 [1:03:39<26:34,  3.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 531: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 775/962 [1:36:23<41:31, 13.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 775: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 962/962 [2:01:34<00:00,  7.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responses saved to ../data/BaseModelResponses/responses.csv\n",
      "\n",
      "Sample responses to Base Model prompts:\n",
      "                                         base_prompt  \\\n",
      "0  Create a list of five (5) unique one-pot veget...   \n",
      "1  Analyze the provided blog content about choosi...   \n",
      "2  Write a Java program that implements the Merge...   \n",
      "\n",
      "                                            response  \n",
      "0  ***\\n\\n**1. Lemon Garlic Spinach Pasta**\\n\\nTh...  \n",
      "1  Here's a breakdown of the analysis:\\n\\n**1. Pr...  \n",
      "2  ```java\\n// Java program to implement Merge So...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "def get_response(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Process the prompts from Flash2.0Responses\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/BaseModelResponses/improved_prompts.csv\")\n",
    "\n",
    "# Rename the columns to match our expected format\n",
    "prompts_df = prompts_df.rename(columns={'base_response': 'base_prompt'})\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['response'] = None\n",
    "\n",
    "# Process each prompt and get a response\n",
    "print(\"\\nProcessing BaseModelResponses prompts:\")\n",
    "for i in tqdm(range(len(prompts_df))):\n",
    "    try:\n",
    "        # Get the response for the base prompt\n",
    "        response = get_response(prompts_df.loc[i, 'base_prompt'])\n",
    "        prompts_df.loc[i, 'response'] = response\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        prompts_df.loc[i, 'response'] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Wait a bit longer if there's an error\n",
    "        time.sleep(2)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/BaseModelResponses/responses.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/BaseModelResponses/responses.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to Base Model prompts:\")\n",
    "print(prompts_df[['base_prompt', 'response']].head(3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing fine-tuned prompts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/962 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 744/962 [1:39:20<25:37,  7.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 776/962 [1:43:57<28:10,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 962/962 [2:10:30<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responses saved to ../data/FineTunedV2Responses/responses.csv\n",
      "\n",
      "Sample responses to fine-tuned prompts:\n",
      "                              fine_tuned_instruction  \\\n",
      "0  Generate a numbered list of 5-7 one-pot vegeta...   \n",
      "1  Analyze the provided text [insert or reference...   \n",
      "2  Generate a complete and runnable Java program ...   \n",
      "\n",
      "                                            response  \n",
      "0  Here are 6 one-pot vegetarian pasta recipes, d...  \n",
      "1  1.  **Estimated Searcher Profile & Goal:** An ...  \n",
      "2  ```java\\nimport java.util.Arrays;\\n\\npublic cl...  \n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "def get_response(instruction, context=None):\n",
    "    if context is not None and not pd.isna(context):\n",
    "        prompt = f\"{instruction}\\n{context}\"\n",
    "    else:\n",
    "        prompt = instruction\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/FineTunedV2Responses/improved_prompts.csv\")\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['response'] = None\n",
    "\n",
    "# Define a function to apply to each row\n",
    "def process_row(row):\n",
    "    try:\n",
    "        # Get the response using both instruction and context\n",
    "        response = get_response(row['fine_tuned_instruction'], row['context'])\n",
    "        time.sleep(0.5)  # Add delay to avoid rate limiting\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {e}\")\n",
    "        time.sleep(2)  # Wait longer if there's an error\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Process each prompt and get a response using apply\n",
    "print(\"\\nProcessing fine-tuned prompts:\")\n",
    "tqdm.pandas()\n",
    "prompts_df['response'] = prompts_df.progress_apply(process_row, axis=1)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/FineTunedV2Responses/responses.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/FineTunedV2Responses/responses.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to fine-tuned prompts:\")\n",
    "print(prompts_df[['fine_tuned_instruction', 'response']].head(3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.genai' has no attribute 'GenerativeModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerativeModel\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(prompt):\n\u001b[32m      3\u001b[39m     response = model.generate_content(prompt)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'google.genai' has no attribute 'GenerativeModel'"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "def get_response(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Process the prompts from ShareGPT\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/Prompt_Training_2.0/seperated_test_data.csv\")\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['response'] = None\n",
    "\n",
    "# Process each prompt and get a response\n",
    "print(\"\\nProcessing ShareGPT prompts:\")\n",
    "for i in tqdm(range(len(prompts_df))):\n",
    "    try:\n",
    "        # Get the response for the prompt\n",
    "        response = get_response(prompts_df.loc[i, 'original_prompt'])\n",
    "        prompts_df.loc[i, 'response'] = response\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        prompts_df.loc[i, 'response'] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Wait a bit longer if there's an error\n",
    "        time.sleep(2)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/NoModelResponses/responses.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/NoModelResponses/responses.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to ShareGPT prompts:\")\n",
    "print(prompts_df[['original_prompt', 'response']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
