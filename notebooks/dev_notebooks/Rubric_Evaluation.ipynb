{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cce5726-8b29-410b-837c-a8845bb5b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai, os, time\n",
    "import json, pathlib, re, math, random\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n",
    "    \n",
    "os.environ[\"GOOGLE_API_KEY\"] = env_vars[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "#env.json\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "\n",
    "# model.generate_content = retry.Retry(predicate=lambda e: hasattr(e, 'code') and e.code in {429, 503})(model.generate_content)\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"../data/FineTunedResponses/responses_to_fine_tuned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76271502-acf4-4f0b-b194-7fd255fb7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_instructions = '''\n",
    "You are an expert evaluator for AI-generated content.\n",
    "Rate the following response according to this enhanced rubric:\n",
    "\n",
    "- Relevance (1-5): How directly the response addresses the specific query or task.\n",
    "- Accuracy (1-5): Factual correctness and absence of errors or misleading information.\n",
    "- Completeness (1-5): Coverage of all required aspects of the task, with appropriate depth.\n",
    "- Clarity (1-5): Organization, coherence, and ease of understanding.\n",
    "- Creativity (1-5): Originality of approach, uniqueness of insights, and innovative thinking.\n",
    "- Conciseness (1-5): Efficiency of expression without unnecessary verbosity.\n",
    "- Technical Correctness (1-5): For technical responses, accuracy of code, formulas, or technical concepts.\n",
    "- Actionability (1-5): How immediately useful and implementable the response is.\n",
    "\n",
    "Additionally, rate the Task Category (select one):\n",
    "- Informational (seeking factual information)\n",
    "- Creative (requiring original content creation)\n",
    "- Technical (code, technical explanations)\n",
    "- Problem-solving (solutions to specific problems)\n",
    "- Analytical (evaluation or analysis of information)\n",
    "\n",
    "Provide the scores as a JSON object with a weighted overall score:\n",
    "{\n",
    "  \"task_category\": \"Technical\",\n",
    "  \"relevance\": 5,\n",
    "  \"accuracy\": 4,\n",
    "  \"completeness\": 5,\n",
    "  \"clarity\": 4,\n",
    "  \"creativity\": 3,\n",
    "  \"conciseness\": 4,\n",
    "  \"technical_correctness\": 5,\n",
    "  \"actionability\": 5,\n",
    "  \"weighted_score\": 4.6\n",
    "}\n",
    "\n",
    "The weighted_score should be calculated based on task category:\n",
    "- Informational: relevance*0.25 + accuracy*0.3 + completeness*0.2 + clarity*0.15 + conciseness*0.1\n",
    "- Creative: relevance*0.2 + completeness*0.15 + clarity*0.15 + creativity*0.4 + conciseness*0.1\n",
    "- Technical: relevance*0.15 + accuracy*0.2 + completeness*0.15 + clarity*0.15 + technical_correctness*0.25 + actionability*0.1\n",
    "- Problem-solving: relevance*0.2 + accuracy*0.15 + completeness*0.15 + clarity*0.1 + actionability*0.4\n",
    "- Analytical: relevance*0.2 + accuracy*0.25 + completeness*0.2 + clarity*0.15 + creativity*0.2\n",
    "\n",
    "Then briefly justify your scores.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9c38d4-bcd0-43b9-9522-363028aa67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_gemini(prompt, response, expected):\n",
    "    full_input = f\"\"\"\n",
    "{rubric_instructions}\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Expected Answer:\n",
    "{expected}\n",
    "\n",
    "Model Response:\n",
    "{response}\n",
    "\"\"\"\n",
    "    result = model.generate_content(full_input)\n",
    "    return result.text\n",
    "\n",
    "# For programmatic access to scores in your analysis:\n",
    "def extract_scores(evaluation_text):\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    # Find JSON object in the evaluation text\n",
    "    json_pattern = r'\\{[\\s\\S]*?\\}'\n",
    "    match = re.search(json_pattern, evaluation_text)\n",
    "    \n",
    "    if match:\n",
    "        try:\n",
    "            scores = json.loads(match.group())\n",
    "            return scores\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0086d6-35bd-4b70-923f-124e52bd24b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create an subsection copy of the sample data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_sample = \u001b[43mdf\u001b[49m.head(\u001b[32m3\u001b[39m).copy()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Use loc (I had to look this up) to set the values\u001b[39;00m\n\u001b[32m      5\u001b[39m df_sample.loc[:, \u001b[33m\"\u001b[39m\u001b[33mevaluation\u001b[39m\u001b[33m\"\u001b[39m] = df_sample.apply(\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: evaluate_with_gemini(\n\u001b[32m      7\u001b[39m         row[\u001b[33m\"\u001b[39m\u001b[33moriginal_prompt\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     ), axis=\u001b[32m1\u001b[39m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an subsection copy of the sample data\n",
    "df_sample = df.head(3).copy()\n",
    "\n",
    "# Use loc (I had to look this up) to set the values\n",
    "df_sample.loc[:, \"evaluation\"] = df_sample.apply(\n",
    "    lambda row: evaluate_with_gemini(\n",
    "        row[\"original_prompt\"],\n",
    "        row[\"response_to_fine_tuned\"],\n",
    "        row[\"fine_tuned_prompt\"]  # Using fine_tuned_prompt as the expected answer\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c41823f6-a81a-44f0-a461-ceccd3696a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          evaluation\n",
      "0  ```json\\n{\\n  \"task_category\": \"Analytical\",\\n...\n",
      "1  ```json\\n{\\n  \"task_category\": \"Analytical\",\\n...\n",
      "2  ```json\\n{\\n  \"task_category\": \"Technical\",\\n ...\n"
     ]
    }
   ],
   "source": [
    "#Prints the subsection to check the values and make sure it looks right\n",
    "print(df_sample[[\"evaluation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37c6e5f1-4c55-4e25-9ec7-3e910b17d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# A function that uses Gemini model to convert the responses into embeddings.\n",
    "# Can generate embeddings for documents or queries depending on if the variable document_mode is set to true or false.\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    document_mode = True  # Specify whether to generate embeddings for documents or queries\n",
    "\n",
    "    def is_retriable(e):\n",
    "        return isinstance(e, exceptions.ServiceUnavailable) or \\\n",
    "               isinstance(e, exceptions.ResourceExhausted) or \\\n",
    "               (hasattr(e, 'code') and e.code in {429, 503})\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embedding_task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n",
    "\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            response = genai.embed_content(\n",
    "                model=\"models/embedding-001\",\n",
    "                content=text,\n",
    "                task_type=embedding_task\n",
    "            )\n",
    "            # Extract just the embedding values from the response (With the call of the embedding row)\n",
    "            embeddings.append(response['embedding'])\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6dcbb65-2499-472f-98ec-6728f97854a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\3316484650.py:8: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from chromadb import Documents, Embeddings\n",
    "\n",
    "# A class that uses the rubric and the embeddings to grade the responses given to it\n",
    "class RubricGrader:\n",
    "    def __init__(self):\n",
    "        self.embedding_function = GeminiEmbeddingFunction()\n",
    "        # Define rubric categories (Im not sure if we should have this as a dictionary or a list, and also if we should have categories for grading as well or just a single rubric)\n",
    "        # Like we will have to find a way to get a rubric for each model, and then get it so tthat we take the model that has the top graded response \n",
    "        self.rubric_prompts = {\n",
    "            \"Excellent\": \"The submission is well-organized, addresses the prompt thoroughly, and provides in-depth analysis with robust supporting evidence.\",\n",
    "            \"Good\": \"The submission addresses most aspects of the prompt with clear points but may lack depth in analysis.\",\n",
    "            \"Average\": \"The submission responds to the prompt in a basic manner, though the analysis and organization are limited.\",\n",
    "            \"Poor\": \"The submission is incomplete, lacks coherence, and does not satisfactorily address the prompt.\"\n",
    "        }\n",
    "        # Precompute embeddings for rubric descriptions\n",
    "        self.rubric_embeddings = self._compute_rubric_embeddings()\n",
    "    \n",
    "    def _compute_rubric_embeddings(self):\n",
    "        descriptions = list(self.rubric_prompts.values())\n",
    "        # Get embeddings for all descriptions at once\n",
    "        embeddings = self.embedding_function(descriptions)\n",
    "        return {grade: emb for grade, emb in zip(self.rubric_prompts.keys(), embeddings)}\n",
    "    \n",
    "    def grade_submission(self, submission_prompt: str):\n",
    "        # Get embedding for the submission\n",
    "        submission_embedding = self.embedding_function([submission_prompt])[0]\n",
    "        \n",
    "        best_grade = None\n",
    "        highest_similarity = -1\n",
    "        \n",
    "        # Compare against each rubric embedding\n",
    "        for grade, rubric_embedding in self.rubric_embeddings.items():\n",
    "            sim = cosine_similarity([submission_embedding], [rubric_embedding])[0][0]\n",
    "            if sim > highest_similarity:\n",
    "                highest_similarity = sim\n",
    "                best_grade = grade\n",
    "                \n",
    "        return best_grade, highest_similarity\n",
    "\n",
    "# Call the grader\n",
    "grader = RubricGrader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fadc6",
   "metadata": {},
   "source": [
    "### The cell below evaluates prompts using a rubric-based grading system with embeddings generated by Gemini. The system works by:\n",
    "\n",
    "1. **Rubric Definition**\n",
    "   - Defines 4 grade levels: Excellent, Good, Average, and Poor\n",
    "   - Each grade has a detailed description of what constitutes that level of quality\n",
    "   - These descriptions serve as reference points for grading\n",
    "\n",
    "2. **Embedding Generation**\n",
    "   - Uses Google's Gemini model to convert both:\n",
    "     - The rubric descriptions into numerical vectors (embeddings)\n",
    "     - Each prompt into its own embedding\n",
    "\n",
    "3. **Similarity Scoring**\n",
    "   - Compares each prompt's embedding to the rubric embeddings using cosine similarity\n",
    "   - Scores range from 0 (completely different) to 1 (identical)\n",
    "   - The grade with the highest similarity score is assigned to the prompt\n",
    "\n",
    "4. **Batch Processing**\n",
    "   - Processes prompts in batches of 10 to manage memory and API usage\n",
    "   - Includes error handling for invalid or empty prompts\n",
    "   - Saves progress periodically to prevent data loss\n",
    "\n",
    "### Understanding the Results:\n",
    "\n",
    "The output includes:\n",
    "- Grade distribution showing how many prompts fall into each category\n",
    "- Average similarity scores for each grade level\n",
    "- Sample graded prompts with their scores\n",
    "- Statistics about processed and skipped prompts\n",
    "\n",
    "#### Interpreting Similarity Scores\n",
    "- Higher scores (closer to 1.0) indicate stronger matches with the rubric criteria\n",
    "- Scores typically fall between 0.6 and 0.8 (Can be much better with more training)\n",
    "- Small differences in scores can be meaningful due to the high-dimensional nature of embeddings\n",
    "\n",
    "#### Limitations/What needs improvement\n",
    "- The system relies on semantic similarity, which may not capture all aspects of quality\n",
    "- Very short or very long prompts may be graded differently due to noise in the text (Preprocessing hopefully fixes but no guarantee)\n",
    "- The quality of grading depends on how well the rubric descriptions differentiate between grades, should be well but may have to train it better still\n",
    "- Need to get a way to follow similar strategy with all models for future comparison, without it taking too long\n",
    "   - End goal is to be able to have grading good enough to where each model has qualities its best at and can clearly be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e72139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\458185774.py:7: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n",
      "Processing batches: 100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grade Distribution:\n",
      "grade\n",
      "Poor         60\n",
      "Good         16\n",
      "Excellent    14\n",
      "Average      10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average Similarity Score by Grade:\n",
      "grade\n",
      "Average      0.709245\n",
      "Excellent    0.687306\n",
      "Good         0.754996\n",
      "Poor         0.737981\n",
      "Name: similarity_score, dtype: float32\n",
      "\n",
      "Final results saved to prompt_grading_results_final.csv\n",
      "\n",
      "Sample of graded prompts:\n",
      "    original_id                                             prompt      grade  \\\n",
      "94        89757  Now, you need to act both as an interviewer an...  Excellent   \n",
      "35        16382  give me a terraform module that will run sever...       Poor   \n",
      "26        54555   Could you write cucumber scenarios for such app?       Good   \n",
      "60       175882       Create a good improv comedy script on crypto       Poor   \n",
      "6         73273  I have a complex task for you\\r\\n\\r\\nSeparate ...       Poor   \n",
      "\n",
      "    similarity_score  \n",
      "94          0.658958  \n",
      "35          0.671703  \n",
      "26          0.779992  \n",
      "60          0.784713  \n",
      "6           0.802017  \n",
      "\n",
      "Total prompts processed: 100\n",
      "Total prompts in dataset: 100\n",
      "Skipped prompts: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/ShareGPT/separated_prompts_clean.csv').head(100)\n",
    "\n",
    "# Initialize the grader\n",
    "grader = RubricGrader()\n",
    "\n",
    "# Create lists to store results\n",
    "results = []\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 10\n",
    "SLEEP_TIME = .25  # seconds\n",
    "\n",
    "# Process prompts in batches\n",
    "for start_idx in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_df = df.iloc[start_idx:start_idx + BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    \n",
    "    # Process each prompt in the current batch\n",
    "    for idx, row in batch_df.iterrows():\n",
    "        prompt = row['prompt']\n",
    "        \n",
    "        # Skip NaN or empty prompts\n",
    "        if pd.isna(prompt) or not isinstance(prompt, str):\n",
    "            print(f\"Skipping prompt {row['original_id']}: Invalid prompt value\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            grade, similarity = grader.grade_submission(prompt)\n",
    "            batch_results.append({\n",
    "                'original_id': row['original_id'],\n",
    "                'prompt': prompt,\n",
    "                'grade': grade,\n",
    "                'similarity_score': similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {row['original_id']}: {str(e)}\")\n",
    "    \n",
    "    # Add batch results to main results list\n",
    "    results.extend(batch_results)\n",
    "    \n",
    "    # Save intermediate results after each batch\n",
    "    if len(results) % (BATCH_SIZE * 10) == 0:  # Save every 10 batches\n",
    "        interim_df = pd.DataFrame(results)\n",
    "        interim_df.to_csv(f'prompt_grading_results_interim_{len(results)}.csv', index=False)\n",
    "    \n",
    "    # Sleep between batches to avoid rate limits\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nGrade Distribution:\")\n",
    "print(results_df['grade'].value_counts())\n",
    "\n",
    "print(\"\\nAverage Similarity Score by Grade:\")\n",
    "print(results_df.groupby('grade')['similarity_score'].mean())\n",
    "\n",
    "# Save final results\n",
    "results_df.to_csv('prompt_grading_results_final.csv', index=False)\n",
    "print(\"\\nFinal results saved to prompt_grading_results_final.csv\")\n",
    "\n",
    "# Display sample of results\n",
    "print(\"\\nSample of graded prompts:\")\n",
    "print(results_df.sample(5))\n",
    "\n",
    "# Print statistics about skipped prompts\n",
    "print(\"\\nTotal prompts processed:\", len(results))\n",
    "print(\"Total prompts in dataset:\", len(df))\n",
    "print(\"Skipped prompts:\", len(df) - len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5817128",
   "metadata": {},
   "source": [
    "## Analyzing responses for best attributs in certain categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02b279a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ResponseEvaluator:\n",
    "    def __init__(self):\n",
    "        self.embedding_function = GeminiEmbeddingFunction()\n",
    "        \n",
    "        # Define rubric descriptions for each category and level\n",
    "        self.category_rubrics = {\n",
    "            'Relevance': {\n",
    "                5: \"The response directly and fully addresses all aspects of the prompt with perfect alignment\",\n",
    "                4: \"The response addresses the main aspects of the prompt with good alignment\",\n",
    "                3: \"The response somewhat addresses the prompt but may have some tangential content\",\n",
    "                2: \"The response only partially addresses the prompt with significant deviation\",\n",
    "                1: \"The response barely addresses or misses the point of the prompt entirely\"\n",
    "            },\n",
    "            'Accuracy': {\n",
    "                5: \"The response is completely factual with precise, verifiable information\",\n",
    "                4: \"The response is mostly accurate with minor imprecisions\",\n",
    "                3: \"The response has a mix of accurate and questionable information\",\n",
    "                2: \"The response contains several inaccuracies\",\n",
    "                1: \"The response is largely incorrect or misleading\"\n",
    "            },\n",
    "            'Completeness': {\n",
    "                5: \"The response comprehensively covers all required elements with additional valuable insights\",\n",
    "                4: \"The response covers all required elements thoroughly\",\n",
    "                3: \"The response covers most required elements with some gaps\",\n",
    "                2: \"The response misses several required elements\",\n",
    "                1: \"The response is significantly incomplete\"\n",
    "            },\n",
    "            'Clarity': {\n",
    "                5: \"The response is exceptionally well-structured, clear, and easy to understand\",\n",
    "                4: \"The response is well-organized and clearly expressed\",\n",
    "                3: \"The response is generally clear but could be better organized\",\n",
    "                2: \"The response is somewhat unclear or poorly structured\",\n",
    "                1: \"The response is confusing and poorly organized\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Pre-compute embeddings for all rubric descriptions\n",
    "        self.category_embeddings = self._compute_rubric_embeddings()\n",
    "    \n",
    "    def _compute_rubric_embeddings(self):\n",
    "        embeddings = {}\n",
    "        for category, levels in self.category_rubrics.items():\n",
    "            category_emb = {}\n",
    "            for level, description in levels.items():\n",
    "                emb = self.embedding_function([description])[0]\n",
    "                category_emb[level] = emb\n",
    "            embeddings[category] = category_emb\n",
    "        return embeddings\n",
    "    \n",
    "    def evaluate_response(self, response: str):\n",
    "        # Get embedding for the response\n",
    "        response_embedding = self.embedding_function([response])[0]\n",
    "        \n",
    "        # Evaluate each category\n",
    "        scores = {}\n",
    "        similarities = {}\n",
    "        for category, level_embeddings in self.category_embeddings.items():\n",
    "            # Compare with each level in the category\n",
    "            level_scores = {}\n",
    "            for level, level_emb in level_embeddings.items():\n",
    "                sim = cosine_similarity([response_embedding], [level_emb])[0][0]\n",
    "                level_scores[level] = sim\n",
    "            \n",
    "            # Get the best matching level\n",
    "            best_level = max(level_scores.items(), key=lambda x: x[1])\n",
    "            scores[category] = best_level[0]\n",
    "            similarities[category] = best_level[1]\n",
    "        \n",
    "        return {\n",
    "            'scores': scores,\n",
    "            'similarities': similarities,\n",
    "            'overall_score': np.mean(list(scores.values())),\n",
    "            'strongest_category': max(similarities.items(), key=lambda x: x[1])[0],\n",
    "            'weakest_category': min(similarities.items(), key=lambda x: x[1])[0]\n",
    "        }\n",
    "\n",
    "def evaluate_responses(responses: list, prompts: list = None):\n",
    "    \"\"\"\n",
    "    Evaluate multiple responses and return the best one\n",
    "    \"\"\"\n",
    "    evaluator = ResponseEvaluator()\n",
    "    results = []\n",
    "    \n",
    "    for i, response in enumerate(responses):\n",
    "        result = evaluator.evaluate_response(response)\n",
    "        result['response'] = response\n",
    "        result['prompt'] = prompts[i] if prompts else None\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by overall score\n",
    "    results.sort(key=lambda x: x['overall_score'], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'best_response': results[0],\n",
    "        'all_results': results,\n",
    "        'summary': {\n",
    "            'average_score': np.mean([r['overall_score'] for r in results]),\n",
    "            'score_distribution': {\n",
    "                category: np.mean([r['scores'][category] for r in results])\n",
    "                for category in results[0]['scores'].keys()\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96eefb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\694891130.py:6: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch best response score: 5.00\n",
      "Batch average score: 3.99\n",
      "\n",
      "Processing batch 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\694891130.py:6: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch best response score: 5.00\n",
      "Batch average score: 3.86\n",
      "\n",
      "Processing batch 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\694891130.py:6: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch best response score: 5.00\n",
      "Batch average score: 4.20\n",
      "\n",
      "Processing batch 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\694891130.py:6: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch best response score: 4.75\n",
      "Batch average score: 3.88\n",
      "\n",
      "Processing batch 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\694891130.py:6: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch best response score: 5.00\n",
      "Batch average score: 4.05\n",
      "\n",
      "=== Final Results ===\n",
      "\n",
      "Best Overall Response:\n",
      "Overall Score: 5.00\n",
      "\n",
      "Category Scores:\n",
      "Relevance: 5\n",
      "Accuracy: 5\n",
      "Completeness: 5\n",
      "Clarity: 5\n",
      "\n",
      "Overall Summary:\n",
      "Average Score: 4.00\n",
      "\n",
      "Category Averages:\n",
      "Relevance: 3.25\n",
      "Accuracy: 4.46\n",
      "Completeness: 3.41\n",
      "Clarity: 4.86\n",
      "\n",
      "Final results saved to response_evaluation_results_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/FineTunedResponses/responses_to_fine_tuned.csv').head(100)\n",
    "\n",
    "# Define batch parameters\n",
    "BATCH_SIZE = 20\n",
    "SLEEP_TIME = .2  # seconds between batches\n",
    "\n",
    "# Initialize empty list for all results\n",
    "all_batch_results = []\n",
    "\n",
    "# Process in batches\n",
    "for start_idx in range(0, len(df), BATCH_SIZE):\n",
    "    print(f\"\\nProcessing batch {start_idx//BATCH_SIZE + 1} of {(len(df) + BATCH_SIZE - 1)//BATCH_SIZE}\")\n",
    "    \n",
    "    # Get current batch\n",
    "    batch_df = df.iloc[start_idx:start_idx + BATCH_SIZE]\n",
    "    \n",
    "    # Prepare responses and prompts for this batch\n",
    "    batch_responses = batch_df['response_to_fine_tuned'].tolist()\n",
    "    batch_prompts = batch_df['original_prompt'].tolist()\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the batch\n",
    "        batch_results = evaluate_responses(batch_responses, batch_prompts)\n",
    "        all_batch_results.append(batch_results)\n",
    "        \n",
    "        # Print batch summary\n",
    "        print(f\"Batch best response score: {batch_results['best_response']['overall_score']:.2f}\")\n",
    "        print(f\"Batch average score: {batch_results['summary']['average_score']:.2f}\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        batch_results_df = pd.DataFrame([{\n",
    "            'original_id': batch_df.iloc[i]['original_id'],\n",
    "            'original_prompt': batch_df.iloc[i]['original_prompt'],\n",
    "            'response': result['response'],\n",
    "            'overall_score': result['overall_score'],\n",
    "            **{f'score_{category}': score for category, score in result['scores'].items()},\n",
    "            'strongest_category': result['strongest_category'],\n",
    "            'weakest_category': result['weakest_category']\n",
    "        } for i, result in enumerate(batch_results['all_results'])])\n",
    "        \n",
    "       #batch_results_df.to_csv(f'response_evaluation_results_batch_{start_idx//BATCH_SIZE + 1}.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {start_idx//BATCH_SIZE + 1}: {str(e)}\")\n",
    "    \n",
    "    # Sleep between batches\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "# Combine all results\n",
    "all_results = []\n",
    "all_responses = []\n",
    "for batch_result in all_batch_results:\n",
    "    all_results.extend(batch_result['all_results'])\n",
    "    \n",
    "# Calculate overall statistics\n",
    "overall_results = {\n",
    "    'best_response': max(all_results, key=lambda x: x['overall_score']),\n",
    "    'all_results': all_results,\n",
    "    'summary': {\n",
    "        'average_score': sum(r['overall_score'] for r in all_results) / len(all_results),\n",
    "        'score_distribution': {\n",
    "            category: sum(r['scores'][category] for r in all_results) / len(all_results)\n",
    "            for category in all_results[0]['scores'].keys()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"\\nBest Overall Response:\")\n",
    "print(f\"Overall Score: {overall_results['best_response']['overall_score']:.2f}\")\n",
    "print(\"\\nCategory Scores:\")\n",
    "for category, score in overall_results['best_response']['scores'].items():\n",
    "    print(f\"{category}: {score}\")\n",
    "\n",
    "print(\"\\nOverall Summary:\")\n",
    "print(f\"Average Score: {overall_results['summary']['average_score']:.2f}\")\n",
    "print(\"\\nCategory Averages:\")\n",
    "for category, score in overall_results['summary']['score_distribution'].items():\n",
    "    print(f\"{category}: {score:.2f}\")\n",
    "\n",
    "# Save final combined results\n",
    "final_results_df = pd.DataFrame([{\n",
    "    'original_id': df.iloc[all_results.index(result)]['original_id'],\n",
    "    'original_prompt': df.iloc[all_results.index(result)]['original_prompt'],\n",
    "    'response': result['response'],\n",
    "    'overall_score': result['overall_score'],\n",
    "    **{f'score_{category}': score for category, score in result['scores'].items()},\n",
    "    'strongest_category': result['strongest_category'],\n",
    "    'weakest_category': result['weakest_category']\n",
    "} for result in all_results])\n",
    "\n",
    "final_results_df.to_csv('response_evaluation_results_final.csv', index=False)\n",
    "print(\"\\nFinal results saved to response_evaluation_results_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5659b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
