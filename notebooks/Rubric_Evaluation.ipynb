{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cce5726-8b29-410b-837c-a8845bb5b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "# Load the model responses and prompts\n",
    "df = pd.read_csv(\"../data/FineTunedResponses/responses_to_fine_tuned.csv\")\n",
    "\n",
    "with open('../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14d7d20-18b2-4422-996c-bf731a1ab3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = env_vars[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "#env.json\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "#retry\n",
    "model.generate_content = retry.Retry(predicate=lambda e: hasattr(e, 'code') and e.code in {429, 503})(model.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76271502-acf4-4f0b-b194-7fd255fb7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Rubric For Evaluation of Responses\n",
    "rubric_instructions = '''\n",
    "You are an expert evaluator for AI-generated content.\n",
    "Rate the following response according to this rubric:\n",
    "\n",
    "- Relevance (1-5): How well the response addresses the prompt.\n",
    "- Accuracy (1-5): Is the response factual and correct?\n",
    "- Completeness (1-5): Are all required parts of the task addressed?\n",
    "- Clarity (1-5): Is the language coherent and well-structured?\n",
    "\n",
    "Provide the scores as a JSON object like this:\n",
    "{\n",
    "  \"relevance\": 5,\n",
    "  \"accuracy\": 4,\n",
    "  \"completeness\": 5,\n",
    "  \"clarity\": 4\n",
    "}\n",
    "\n",
    "Then briefly justify your scores.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e9c38d4-bcd0-43b9-9522-363028aa67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the rubric and the response and returns the evaluation\n",
    "def evaluate_with_gemini(prompt, response, expected):\n",
    "    full_input = f\"\"\"\n",
    "{rubric_instructions}\n",
    "\n",
    "Prompt:\n",
    "{prompt}\n",
    "\n",
    "Expected Answer:\n",
    "{expected}\n",
    "\n",
    "Model Response:\n",
    "{response}\n",
    "\"\"\"\n",
    "    result = model.generate_content(full_input)\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0086d6-35bd-4b70-923f-124e52bd24b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an subsection copy of the sample data\n",
    "df_sample = df.head(3).copy()\n",
    "\n",
    "# Use loc (I had to look this up) to set the values\n",
    "df_sample.loc[:, \"evaluation\"] = df_sample.apply(\n",
    "    lambda row: evaluate_with_gemini(\n",
    "        row[\"original_prompt\"],\n",
    "        row[\"response_to_fine_tuned\"],\n",
    "        row[\"fine_tuned_prompt\"]  # Using fine_tuned_prompt as the expected answer\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41823f6-a81a-44f0-a461-ceccd3696a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     original_prompt  \\\n",
      "0   One-pot vegetarian pasta recipes for busy nights   \n",
      "1  We have the following blog content... what is ...   \n",
      "2  how o sort element using merge sort technique ...   \n",
      "\n",
      "                              response_to_fine_tuned  \\\n",
      "0  Okay, let's analyze the strengths and weakness...   \n",
      "1  Okay, let's break down the likely user intent ...   \n",
      "2  ```java\\r\\npublic class MergeSort {\\r\\n\\r\\n   ...   \n",
      "\n",
      "                                          evaluation  \n",
      "0  {\\n  \"relevance\": 5,\\n  \"accuracy\": 4,\\n  \"com...  \n",
      "1  ```json\\n{\\n  \"relevance\": 5,\\n  \"accuracy\": 4...  \n",
      "2  ```json\\n{\\n  \"relevance\": 5,\\n  \"accuracy\": 5...  \n"
     ]
    }
   ],
   "source": [
    "#Prints the subsection to check the values and make sure it looks right\n",
    "print(df_sample[[\"original_prompt\", \"response_to_fine_tuned\", \"evaluation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37c6e5f1-4c55-4e25-9ec7-3e910b17d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# A function that uses Gemini model to convert the responses into embeddings.\n",
    "# Can generate embeddings for documents or queries depending on if the variable document_mode is set to true or false.\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    document_mode = True  # Specify whether to generate embeddings for documents or queries\n",
    "\n",
    "    def is_retriable(e):\n",
    "        return isinstance(e, exceptions.ServiceUnavailable) or \\\n",
    "               isinstance(e, exceptions.ResourceExhausted) or \\\n",
    "               (hasattr(e, 'code') and e.code in {429, 503})\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embedding_task = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n",
    "\n",
    "        # Handle single string or list of strings\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        \n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            response = genai.embed_content(\n",
    "                model=\"models/embedding-001\",\n",
    "                content=text,\n",
    "                task_type=embedding_task\n",
    "            )\n",
    "            # Extract just the embedding values from the response (With the call of the embedding row)\n",
    "            embeddings.append(response['embedding'])\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6dcbb65-2499-472f-98ec-6728f97854a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\3316484650.py:8: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from chromadb import Documents, Embeddings\n",
    "\n",
    "# A class that uses the rubric and the embeddings to grade the responses given to it\n",
    "class RubricGrader:\n",
    "    def __init__(self):\n",
    "        self.embedding_function = GeminiEmbeddingFunction()\n",
    "        # Define rubric categories (Im not sure if we should have this as a dictionary or a list, and also if we should have categories for grading as well or just a single rubric)\n",
    "        # Like we will have to find a way to get a rubric for each model, and then get it so tthat we take the model that has the top graded response \n",
    "        self.rubric_prompts = {\n",
    "            \"Excellent\": \"The submission is well-organized, addresses the prompt thoroughly, and provides in-depth analysis with robust supporting evidence.\",\n",
    "            \"Good\": \"The submission addresses most aspects of the prompt with clear points but may lack depth in analysis.\",\n",
    "            \"Average\": \"The submission responds to the prompt in a basic manner, though the analysis and organization are limited.\",\n",
    "            \"Poor\": \"The submission is incomplete, lacks coherence, and does not satisfactorily address the prompt.\"\n",
    "        }\n",
    "        # Precompute embeddings for rubric descriptions\n",
    "        self.rubric_embeddings = self._compute_rubric_embeddings()\n",
    "    \n",
    "    def _compute_rubric_embeddings(self):\n",
    "        descriptions = list(self.rubric_prompts.values())\n",
    "        # Get embeddings for all descriptions at once\n",
    "        embeddings = self.embedding_function(descriptions)\n",
    "        return {grade: emb for grade, emb in zip(self.rubric_prompts.keys(), embeddings)}\n",
    "    \n",
    "    def grade_submission(self, submission_prompt: str):\n",
    "        # Get embedding for the submission\n",
    "        submission_embedding = self.embedding_function([submission_prompt])[0]\n",
    "        \n",
    "        best_grade = None\n",
    "        highest_similarity = -1\n",
    "        \n",
    "        # Compare against each rubric embedding\n",
    "        for grade, rubric_embedding in self.rubric_embeddings.items():\n",
    "            sim = cosine_similarity([submission_embedding], [rubric_embedding])[0][0]\n",
    "            if sim > highest_similarity:\n",
    "                highest_similarity = sim\n",
    "                best_grade = grade\n",
    "                \n",
    "        return best_grade, highest_similarity\n",
    "\n",
    "# Call the grader\n",
    "grader = RubricGrader()\n",
    "\n",
    "# Below is a small example if needed\n",
    "\n",
    "# # Example submissions to go through the grader (Have to get it so we can go through all the responses, and then select the top ones)\n",
    "# example_prompts = [\n",
    "#     \"The paper offers an insightful discussion with robust evidence, clearly meeting and exceeding the criteria.\",\n",
    "#     \"The response is decent and clearly structured, though some points need more elaboration.\",\n",
    "#     \"A superficial attempt with limited analysis and organization.\",\n",
    "#     \"There is minimal content provided, lacking coherence, focus, and clear arguments.\"\n",
    "# ]\n",
    "\n",
    "# # Process and display grading for each example\n",
    "# for prompt in example_prompts:\n",
    "#     grade, similarity = grader.grade_submission(prompt)\n",
    "#     print(f\"Submission Prompt: {prompt}\")\n",
    "#     print(f\"Assigned Grade: {grade} (Similarity Score: {similarity:.2f})\")\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641fadc6",
   "metadata": {},
   "source": [
    "### The cell below evaluates prompts using a rubric-based grading system with embeddings generated by Gemini. The system works by:\n",
    "\n",
    "1. **Rubric Definition**\n",
    "   - Defines 4 grade levels: Excellent, Good, Average, and Poor\n",
    "   - Each grade has a detailed description of what constitutes that level of quality\n",
    "   - These descriptions serve as reference points for grading\n",
    "\n",
    "2. **Embedding Generation**\n",
    "   - Uses Google's Gemini model to convert both:\n",
    "     - The rubric descriptions into numerical vectors (embeddings)\n",
    "     - Each prompt into its own embedding\n",
    "\n",
    "3. **Similarity Scoring**\n",
    "   - Compares each prompt's embedding to the rubric embeddings using cosine similarity\n",
    "   - Scores range from 0 (completely different) to 1 (identical)\n",
    "   - The grade with the highest similarity score is assigned to the prompt\n",
    "\n",
    "4. **Batch Processing**\n",
    "   - Processes prompts in batches of 10 to manage memory and API usage\n",
    "   - Includes error handling for invalid or empty prompts\n",
    "   - Saves progress periodically to prevent data loss\n",
    "\n",
    "### Understanding the Results:\n",
    "\n",
    "The output includes:\n",
    "- Grade distribution showing how many prompts fall into each category\n",
    "- Average similarity scores for each grade level\n",
    "- Sample graded prompts with their scores\n",
    "- Statistics about processed and skipped prompts\n",
    "\n",
    "#### Interpreting Similarity Scores\n",
    "- Higher scores (closer to 1.0) indicate stronger matches with the rubric criteria\n",
    "- Scores typically fall between 0.6 and 0.8 (Can be much better with more training)\n",
    "- Small differences in scores can be meaningful due to the high-dimensional nature of embeddings\n",
    "\n",
    "#### Limitations/What needs improvement\n",
    "- The system relies on semantic similarity, which may not capture all aspects of quality\n",
    "- Very short or very long prompts may be graded differently due to noise in the text (Preprocessing hopefully fixes but no guarantee)\n",
    "- The quality of grading depends on how well the rubric descriptions differentiate between grades, should be well but may have to train it better still\n",
    "- Need to get a way to follow similar strategy with all models for future comparison, without it taking too long\n",
    "   - End goal is to be able to have grading good enough to where each model has qualities its best at and can clearly be chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e72139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\458185774.py:7: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n",
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grade Distribution:\n",
      "grade\n",
      "Poor         60\n",
      "Good         16\n",
      "Excellent    14\n",
      "Average      10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average Similarity Score by Grade:\n",
      "grade\n",
      "Average      0.709245\n",
      "Excellent    0.687306\n",
      "Good         0.754996\n",
      "Poor         0.737981\n",
      "Name: similarity_score, dtype: float32\n",
      "\n",
      "Final results saved to prompt_grading_results_final.csv\n",
      "\n",
      "Sample of graded prompts:\n",
      "    original_id                                             prompt      grade  \\\n",
      "94        89757  Now, you need to act both as an interviewer an...  Excellent   \n",
      "35        16382  give me a terraform module that will run sever...       Poor   \n",
      "26        54555   Could you write cucumber scenarios for such app?       Good   \n",
      "60       175882       Create a good improv comedy script on crypto       Poor   \n",
      "6         73273  I have a complex task for you\\r\\n\\r\\nSeparate ...       Poor   \n",
      "\n",
      "    similarity_score  \n",
      "94          0.658958  \n",
      "35          0.671703  \n",
      "26          0.779992  \n",
      "60          0.784713  \n",
      "6           0.802017  \n",
      "\n",
      "Total prompts processed: 100\n",
      "Total prompts in dataset: 100\n",
      "Skipped prompts: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/ShareGPT/separated_prompts_clean.csv').head(100)\n",
    "\n",
    "# Initialize the grader\n",
    "grader = RubricGrader()\n",
    "\n",
    "# Create lists to store results\n",
    "results = []\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 10\n",
    "SLEEP_TIME = .25  # seconds\n",
    "\n",
    "# Process prompts in batches\n",
    "for start_idx in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_df = df.iloc[start_idx:start_idx + BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    \n",
    "    # Process each prompt in the current batch\n",
    "    for idx, row in batch_df.iterrows():\n",
    "        prompt = row['prompt']\n",
    "        \n",
    "        # Skip NaN or empty prompts\n",
    "        if pd.isna(prompt) or not isinstance(prompt, str):\n",
    "            print(f\"Skipping prompt {row['original_id']}: Invalid prompt value\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            grade, similarity = grader.grade_submission(prompt)\n",
    "            batch_results.append({\n",
    "                'original_id': row['original_id'],\n",
    "                'prompt': prompt,\n",
    "                'grade': grade,\n",
    "                'similarity_score': similarity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {row['original_id']}: {str(e)}\")\n",
    "    \n",
    "    # Add batch results to main results list\n",
    "    results.extend(batch_results)\n",
    "    \n",
    "    # Save intermediate results after each batch\n",
    "    if len(results) % (BATCH_SIZE * 10) == 0:  # Save every 10 batches\n",
    "        interim_df = pd.DataFrame(results)\n",
    "        interim_df.to_csv(f'prompt_grading_results_interim_{len(results)}.csv', index=False)\n",
    "    \n",
    "    # Sleep between batches to avoid rate limits\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "# Convert all results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nGrade Distribution:\")\n",
    "print(results_df['grade'].value_counts())\n",
    "\n",
    "print(\"\\nAverage Similarity Score by Grade:\")\n",
    "print(results_df.groupby('grade')['similarity_score'].mean())\n",
    "\n",
    "# Save final results\n",
    "results_df.to_csv('prompt_grading_results_final.csv', index=False)\n",
    "print(\"\\nFinal results saved to prompt_grading_results_final.csv\")\n",
    "\n",
    "# Display sample of results\n",
    "print(\"\\nSample of graded prompts:\")\n",
    "print(results_df.sample(5))\n",
    "\n",
    "# Print statistics about skipped prompts\n",
    "print(\"\\nTotal prompts processed:\", len(results))\n",
    "print(\"Total prompts in dataset:\", len(df))\n",
    "print(\"Skipped prompts:\", len(df) - len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5817128",
   "metadata": {},
   "source": [
    "## Analyzing responses for best attributs in certain categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02b279a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ResponseEvaluator:\n",
    "    def __init__(self):\n",
    "        self.embedding_function = GeminiEmbeddingFunction()\n",
    "        \n",
    "        # Define rubric descriptions for each category and level\n",
    "        self.category_rubrics = {\n",
    "            'Relevance': {\n",
    "                5: \"The response directly and fully addresses all aspects of the prompt with perfect alignment\",\n",
    "                4: \"The response addresses the main aspects of the prompt with good alignment\",\n",
    "                3: \"The response somewhat addresses the prompt but may have some tangential content\",\n",
    "                2: \"The response only partially addresses the prompt with significant deviation\",\n",
    "                1: \"The response barely addresses or misses the point of the prompt entirely\"\n",
    "            },\n",
    "            'Accuracy': {\n",
    "                5: \"The response is completely factual with precise, verifiable information\",\n",
    "                4: \"The response is mostly accurate with minor imprecisions\",\n",
    "                3: \"The response has a mix of accurate and questionable information\",\n",
    "                2: \"The response contains several inaccuracies\",\n",
    "                1: \"The response is largely incorrect or misleading\"\n",
    "            },\n",
    "            'Completeness': {\n",
    "                5: \"The response comprehensively covers all required elements with additional valuable insights\",\n",
    "                4: \"The response covers all required elements thoroughly\",\n",
    "                3: \"The response covers most required elements with some gaps\",\n",
    "                2: \"The response misses several required elements\",\n",
    "                1: \"The response is significantly incomplete\"\n",
    "            },\n",
    "            'Clarity': {\n",
    "                5: \"The response is exceptionally well-structured, clear, and easy to understand\",\n",
    "                4: \"The response is well-organized and clearly expressed\",\n",
    "                3: \"The response is generally clear but could be better organized\",\n",
    "                2: \"The response is somewhat unclear or poorly structured\",\n",
    "                1: \"The response is confusing and poorly organized\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Pre-compute embeddings for all rubric descriptions\n",
    "        self.category_embeddings = self._compute_rubric_embeddings()\n",
    "    \n",
    "    def _compute_rubric_embeddings(self):\n",
    "        embeddings = {}\n",
    "        for category, levels in self.category_rubrics.items():\n",
    "            category_emb = {}\n",
    "            for level, description in levels.items():\n",
    "                emb = self.embedding_function([description])[0]\n",
    "                category_emb[level] = emb\n",
    "            embeddings[category] = category_emb\n",
    "        return embeddings\n",
    "    \n",
    "    def evaluate_response(self, response: str):\n",
    "        # Get embedding for the response\n",
    "        response_embedding = self.embedding_function([response])[0]\n",
    "        \n",
    "        # Evaluate each category\n",
    "        scores = {}\n",
    "        similarities = {}\n",
    "        for category, level_embeddings in self.category_embeddings.items():\n",
    "            # Compare with each level in the category\n",
    "            level_scores = {}\n",
    "            for level, level_emb in level_embeddings.items():\n",
    "                sim = cosine_similarity([response_embedding], [level_emb])[0][0]\n",
    "                level_scores[level] = sim\n",
    "            \n",
    "            # Get the best matching level\n",
    "            best_level = max(level_scores.items(), key=lambda x: x[1])\n",
    "            scores[category] = best_level[0]\n",
    "            similarities[category] = best_level[1]\n",
    "        \n",
    "        return {\n",
    "            'scores': scores,\n",
    "            'similarities': similarities,\n",
    "            'overall_score': np.mean(list(scores.values())),\n",
    "            'strongest_category': max(similarities.items(), key=lambda x: x[1])[0],\n",
    "            'weakest_category': min(similarities.items(), key=lambda x: x[1])[0]\n",
    "        }\n",
    "\n",
    "def evaluate_responses(responses: list, prompts: list = None):\n",
    "    \"\"\"\n",
    "    Evaluate multiple responses and return the best one\n",
    "    \"\"\"\n",
    "    evaluator = ResponseEvaluator()\n",
    "    results = []\n",
    "    \n",
    "    for i, response in enumerate(responses):\n",
    "        result = evaluator.evaluate_response(response)\n",
    "        result['response'] = response\n",
    "        result['prompt'] = prompts[i] if prompts else None\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by overall score\n",
    "    results.sort(key=lambda x: x['overall_score'], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'best_response': results[0],\n",
    "        'all_results': results,\n",
    "        'summary': {\n",
    "            'average_score': np.mean([r['overall_score'] for r in results]),\n",
    "            'score_distribution': {\n",
    "                category: np.mean([r['scores'][category] for r in results])\n",
    "                for category in results[0]['scores'].keys()\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96eefb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianra\\AppData\\Local\\Temp\\ipykernel_30136\\694891130.py:6: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  self.embedding_function = GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Response:\n",
      "Original Prompt: One-pot vegetarian pasta recipes for busy nights\n",
      "\n",
      "Text: Okay, let's analyze the strengths and weaknesses of these two one-pot pasta recipes:\n",
      "\n",
      "**General Considerations for Both Recipes:**\n",
      "\n",
      "*   **Strengths (Shared):**\n",
      "    *   **Ease of Preparation:**  Both are designed for simplicity, minimizing dishes and cooking steps. They are ideal for busy weeknights.\n",
      "    *   **Customizable:**  The ingredients listed are broad, allowing for flexibility based on personal preferences and what's on hand.\n",
      "    *   **Budget-Friendly:** One-pot meals often utilize...\n",
      "\n",
      "Overall Score: 5.00\n",
      "\n",
      "Category Scores:\n",
      "Relevance: 5\n",
      "Accuracy: 5\n",
      "Completeness: 5\n",
      "Clarity: 5\n",
      "\n",
      "Strongest in: Relevance\n",
      "Weakest in: Accuracy\n",
      "\n",
      "Overall Summary:\n",
      "Average Score: 4.12\n",
      "\n",
      "Category Averages:\n",
      "Relevance: 3.40\n",
      "Accuracy: 4.60\n",
      "Completeness: 3.60\n",
      "Clarity: 4.90\n",
      "\n",
      "Detailed Results for All Responses:\n",
      "\n",
      "Response 1:\n",
      "Original Prompt: One-pot vegetarian pasta recipes for busy nights\n",
      "Response: Okay, let's analyze the strengths and weaknesses of these two one-pot pasta recipes:\n",
      "\n",
      "**General Considerations for Both Recipes:**\n",
      "\n",
      "*   **Strengths (Shared):**\n",
      "    *   **Ease of Preparation:**  B...\n",
      "Overall Score: 5.00\n",
      "Category Scores:\n",
      "  Relevance: 5\n",
      "  Accuracy: 5\n",
      "  Completeness: 5\n",
      "  Clarity: 5\n",
      "Strongest Category: Relevance\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 2:\n",
      "Original Prompt: We have the following blog content... what is the likely user intent of a searcher who lands on this page? Our goal should be to answer that intent better and faster than anyone else!\n",
      "\n",
      "The content:\n",
      "\n",
      "Consumers want more choices, but all those choices can be daunting. It gives the consumer the responsibility of educating themselves on the best option for their circumstances. On GigSalad, we deliver more than 600 categories of entertainment and event services for the ability to compare rates of various performers. With an enormous variety of services, itâ€™s a really great option for first-timers and seasoned event planners alike. But with so many talented professionals out there, how do you choose? Budget may be a large factor in who you choose to book, but before you make your choice, here are 6 reasons for choosing the best entertainment, not the cheapest.\n",
      "\n",
      "1. You get what you pay for.\n",
      "Entertainers with higher rates oftentimes need to charge more to account for the costs of their high-quality equipment. Many performers set them based on their professional experience. They know what their service is worth and theyâ€™ve been able to make their living from that. Theyâ€™ve invested in their business with the best materials, gear, instruments, props, etc. to give their clients most immersive experience. These materials can make a huge difference between one vendor and another.\n",
      "\n",
      "Say youâ€™re looking for a princess character for your childâ€™s birthday party. To make a fairy tale come to life, performers need costumes and accessories that not only look pretty and authentic but also hold up to wear and tear. Along with the costs of dry cleaning, well-made costumes are a large investment for professional princess characters. If youâ€™re choosing a performer based solely on price, you risk hiring someone who doesnâ€™t provide the same magical experience for your kids, perhaps even giving them a disenchanting experience. A top-notch entertainer will deliver a performance that you and your family will remember for years to come.\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Donâ€™t let the sticker shock of good entertainment throw off your plans. You donâ€™t want always want to break your budget at the expense of the rest of the event. Simply determine your priorities and set the spending from there. Itâ€™s important to research the real cost of a quality service before booking. If there is a significant drop-off between bids, then you can probably expect a drop-off in quality as well. Adjust your budget to your talent and quality expectations.\n",
      "\n",
      "2. Professionals who are invested in their business are also invested in their clients.\n",
      "When performers rely heavily on your business for their income, theyâ€™ll go to great lengths to make sure youâ€™re happy with their service. Most of our GigSalad entertainers are self-employed, independent business people. They depend on good reviews and word of mouth for promotion and buzz.\n",
      "\n",
      "Hiring an experienced professional means youâ€™re hiring someone who has invested their lives into entertaining. Their goal is to make a living and provide you with a memorable event at the same time. Great memories are the foundation of their income and theyâ€™ll often go to great lengths to ensure youâ€™re satisfied.\n",
      "\n",
      "3. Pros know how to customize for your needs.\n",
      "Many low-cost entertainers and performers are at the beginning of their careers or at a lower skill level. Often, this means that they offer a limited repertoire and fewer options as they are developing their craft. Your event is unique to you, therefore, youâ€™ll need someone who can customize their act or services to your event. A good sign of a professional is their ability and willingness to reasonably adapt to your event needs.\n",
      "\n",
      "4. Pros offer more than just entertainment. They give you peace of mind.\n",
      "Professionals who charge more than the competition have the confidence in their experience. If theyâ€™ve got a thousand performances under their belt, theyâ€™ve encountered all kinds of situations, which means theyâ€™d be prepared for anything that may come up at your event. The ability to adapt and improvise is especially beneficial in unexpected situations that put your event at risk of turning into a disaster!\n",
      "\n",
      "\n",
      "5. The difference in cost makes all the difference in the success of your event.\n",
      "Youâ€™ve decided to take the risk and hire an entertainer for your event. Perhaps a strolling magician for the office party you were picked to plan. Youâ€™ve looked through the profiles, read the reviews, and checked out the videos and pictures. When the moment of decision comes, the temptation of saving a few bucks by hiring someone with fewer reviews and a lower quality presentation get very real very quickly. Thatâ€™s money that could go to streamers! But those few dollars could mean the difference in the quality of your event and the likelihood of using entertainment again. A great professional turns the groans of â€œRemember last year? I hope that doesnâ€™t happen again,â€ to the excitement of â€œRemember last year? I canâ€™t wait to see what happens this time!â€ Paying a few extra bucks can prevent the dreaded buyerâ€™s remorse.\n",
      "\n",
      "6. There are no do-overs.\n",
      "GigSaladâ€™s mission is to help you Book Something Awesome. We recognize that many of the events we connect entertainers with are often once in a lifetime moments. That significance is one of the reasons we encourage you to take the time and research your providers well. You may not get a chance to do it again. We hope that you would take the few extra minutes necessary to thoroughly read the bids, examine the profiles, view the media, and make an educated decision. Donâ€™t risk devaluing a critical life moment or celebration by going bargain hunting. If the price is right and the quality is high, invest in the memory.  \n",
      "\n",
      "Weâ€™re not telling you that you should choose only vendors who charge more for their services. Itâ€™s certainly possible to find a talented professional with a lower fee. Weâ€™re simply encouraging you to do your homework. Look at their photos and videos, read reviews, ask them questions. Figure out why their rates are what they are. When youâ€™ve got all the information in front of you, youâ€™ll be able to make an informed decision and confidently book the right person for your event.\n",
      "Response: Alright lads, gather round! We've got a mountain to climb to get FenerbahÃ§e back on top, not just in Turkey, but in Europe. Galatasaray are the benchmark, and we're going to surpass them. Forget the s...\n",
      "Overall Score: 5.00\n",
      "Category Scores:\n",
      "  Relevance: 5\n",
      "  Accuracy: 5\n",
      "  Completeness: 5\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 3:\n",
      "Original Prompt: how o sort element using merge sort technique using array in java Save & SubmitCancel\n",
      "Response: Okay, I understand. I will take your input text, analyze it based on your specific instructions, and provide the requested output. I will pay attention to the specific requirements in your instruction...\n",
      "Overall Score: 4.75\n",
      "Category Scores:\n",
      "  Relevance: 4\n",
      "  Accuracy: 5\n",
      "  Completeness: 5\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 4:\n",
      "Original Prompt: make a javascript class \"GraphicLayer\" which is supplied a parent element, an image element or the url of an image. If the second argument is a url, it will create the image element. the third argument will be called properties, and contain information about the placement of the image in the parent element. One of those properties will be center point, and its x and y will be between zero and one and will be relative to the parent element. (x:.5 and y: .5 will mean the child image is centered on the parent. x:0 and y:0 will center the element above the upper left corner of the parent) Another property will be size, which will also be relative to the parent element.The default center point will be x: 0.5 and y: 0.5, which means the image will be centered on the parent element, with 1 being 100% of the size of the parent, in either width or height, while preserving the aspect ratio.   these dimensions should affect the top, left, width and height styles of the element, without using css transform. there should be a separate method that adjusts those styles.  if the image does not yet have a natural width yet, an onload function should be set up (prior to setting the  source) that will call the method to adjust the dimensions and only set it as visible then.\n",
      "Response: Okay, let's break this down.\n",
      "\n",
      "**Perspective 1: Business Development Manager (BDM)**\n",
      "\n",
      "A BDM is focused on strategic growth, market expansion, and competitive advantage. They need information to ide...\n",
      "Overall Score: 4.50\n",
      "Category Scores:\n",
      "  Relevance: 4\n",
      "  Accuracy: 4\n",
      "  Completeness: 5\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 5:\n",
      "Original Prompt: !Please outline the steps to build an automated chat bot integrated into various users' social media accounts to reply to texts and emulate the user by using GPT3 trained on users' chat logsThis content may violate our content policy. If you believe this to be in error, please submit your feedback â€” your input will aid our research in this area.\n",
      "Response: Okay, here's a Twitter thread announcing the launch of SWR 2.0, ready to copy and paste:\n",
      "\n",
      "---\n",
      "\n",
      "**Tweet 1 (Opening):**\n",
      "\n",
      "SWR 2.0 is HERE! ðŸŽ‰ Blazing fast data fetching just got *even better*. âœ¨ Exp...\n",
      "Overall Score: 4.25\n",
      "Category Scores:\n",
      "  Relevance: 5\n",
      "  Accuracy: 5\n",
      "  Completeness: 2\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 6:\n",
      "Original Prompt: Write a tweet announcing the launch of SWR 2.0 in the style of rauchg\n",
      "\n",
      "Outline the top 5 newest features in a markdown list format, using â—† as the bullet point.\n",
      "Response: Okay, I'm ready. Please provide the comments you want me to clean. I will do my best to extract the core meaning and present it in a concise and clear manner, removing unnecessary elements like timest...\n",
      "Overall Score: 3.75\n",
      "Category Scores:\n",
      "  Relevance: 4\n",
      "  Accuracy: 5\n",
      "  Completeness: 2\n",
      "  Clarity: 4\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 7:\n",
      "Original Prompt: I have a complex task for you\n",
      "Response: Okay, let's break down the likely user intent for this blog post.\n",
      "\n",
      "**1. Topic Analysis:**\n",
      "\n",
      "*   The blog post is clearly about hiring entertainment for events.\n",
      "*   It focuses on the trade-off betw...\n",
      "Overall Score: 3.50\n",
      "Category Scores:\n",
      "  Relevance: 1\n",
      "  Accuracy: 3\n",
      "  Completeness: 5\n",
      "  Clarity: 5\n",
      "Strongest Category: Relevance\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 8:\n",
      "Original Prompt: Below I copied a bunch of comments from a video called AI Inspired Web Design is INSANE! - Fast Tutorial (https://www.youtube.com/watch?v=CQt26KNuGdo). \n",
      "First I want you to clean up this comments and make it more readable. You can remove unnecessary things like '2 days ago'.\n",
      "\n",
      "TaraBoo Art ARMY\n",
      "TaraBoo Art ARMY\n",
      "2 days ago\n",
      "I feel like a great use for this would be in the initial design phase. Like, imagine using AI to create 20 different prototypes to let a client choose from. That would take AGES manually, but just a few minutes in Midjourney. The client chooses one, and you have your starting point.\n",
      "\n",
      "118\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "\n",
      "6 replies\n",
      "South Coast Inventors\n",
      "South Coast Inventors\n",
      "2 days ago\n",
      "Yeah that a good idea preview mode for the client and in a month or so they will have models that actually output non gibberish text.\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Vodoved\n",
      "Vodoved\n",
      "1 day ago\n",
      "so many prototypes for client not a good choice. 2-3 will be better\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "TaraBoo Art ARMY\n",
      "TaraBoo Art ARMY\n",
      "1 day ago\n",
      " @Vodoved  Iâ€™ve been doing freelance design for most of the last 25 years. Iâ€™ve found clients sometimes arenâ€™t happy with just 2 or 3. I used to start with 2 mockups, but about 10% of clients wouldnâ€™t like either and it was frustrating because all those mockups took time. Being able to do 5 or 10 or even 20 in less time than 1 used to take means a greater likelihood that the client will be happy with at least one of them.\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "SuperChaos\n",
      "SuperChaos\n",
      "1 day ago\n",
      " @TaraBoo Art ARMY  10-20 sounds insane, honestly. As a client, having to choose from that would stress me out! lol\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "TaraBoo Art ARMY\n",
      "TaraBoo Art ARMY\n",
      "1 day ago (edited)\n",
      " @SuperChaos  I mean, I wouldnâ€™t do it automatically. Maybe offer 2-3 at first. Or 5, which is what I would offer as an option when I was doing book covers. (Theyâ€™d have to pay more for 5 mockups than for 2.) But for a picky client (and there are lots of those) having more options that you donâ€™t have to spend hours making would be a game changer.\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Anis Arpadzic\n",
      "Anis Arpadzic\n",
      "5 hours ago\n",
      "The client shouldnâ€™t choose, perhaps the client should be persuaded to use the design you created.\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Laone Kwati\n",
      "Laone Kwati\n",
      "1 day ago\n",
      "That's actually great idea to utilize it for colour schemes and ideas then work towards your goal. It is a great tool for assets, sadly some will use it for final work, where they don't even add any creative input except the text prompt.\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "anony mous\n",
      "anony mous\n",
      "2 days ago\n",
      "The various AI designs shown look insanely good.\n",
      "\n",
      "36\n",
      "\n",
      "DesignCourse\n",
      "\n",
      "Reply\n",
      "\n",
      "Peter6th\n",
      "Peter6th\n",
      "2 days ago (edited)\n",
      "I think NN-based image generators have the same issues like Behance and Dribble have - often it looks the same, follows a trend, can't create new things, don't have fresh ideas ... I  like more to incorporate ideas from other mediums, from old website styles (like 2010) or from websites from other parts of the world except western countries. \n",
      "The problem with that is, that once you got this suggestions (from an NN based generator) you are almost become captured by it and can't escape mentally for new ideas.\n",
      "\n",
      "25\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "\n",
      "9 replies\n",
      "shrunkensimon\n",
      "shrunkensimon\n",
      "2 days ago\n",
      "This is what AI is in a nutshell. It can only regurgitate what you feed it, there is no innovation or inspiration.\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Mukomio\n",
      "Mukomio\n",
      "1 day ago (edited)\n",
      " @shrunkensimon  But isn't this just the beginning? I feel like although it is still pretty limited, it has blown people's minds. After the success and media attention, I would be surprised if this wouldn't blow up like crazy and become increasingly sophisticated.\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "shrunkensimon\n",
      "shrunkensimon\n",
      "19 hours ago\n",
      " @Mukomio  All AI relies on material fed to it. If it were to replace too many people then it will only have its own material to feed on, and become completely stagnant.\n",
      "\n",
      "It's hype. Look at self driving cars.. hasn't taken off. The chess playing robot broke the kids hand lol. \n",
      "\n",
      "Plus people want to do business with people, not machines that can't communicate.\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "ib0o0\n",
      "ib0o0\n",
      "7 hours ago\n",
      " @shrunkensimon  you don't have an understanding of how AI works. Creativity is just mixing stuff together, don't kid yourself. People you deem creative have experiences they put together to find new ways of building things. If you give the correct prompts, the AI can generate novelty the same way as people can.\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "shrunkensimon\n",
      "shrunkensimon\n",
      "7 hours ago\n",
      " @ib0o0  A child can mix paints together on a canvas, doesn't mean it's going to be 'good' art. Someone with an understanding of musical notation can write a piano piece, doesn't mean it will sound 'good'.\n",
      "\n",
      "AI has no conception of what works for humans, it has no lateral or emotive thinking.\n",
      "\n",
      "Also creativity is not the same as imagination or inspiration.\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Jaosn Fuller\n",
      "Jaosn Fuller\n",
      "4 hours ago\n",
      "Hopefully you don't use designs from Asian websites.... Most of them are cluttered to hell....which they like for some reason or it's because they arent used to something better.\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Peter6th\n",
      "Peter6th\n",
      "4 hours ago\n",
      " @Jaosn Fuller  I don't use designs, I use ideas :) I use whatever idea I like.\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "May the Science be with You\n",
      "May the Science be with You\n",
      "3 hours ago (edited)\n",
      "It can create new things and it does create new things. The way it looks depends on what you tell it to spit out. Midjourney has some preset style, but you can tell it to look different. You'll have to specifiy what you want. You have really no idea what these image generators are capable of.\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Peter6th\n",
      "Peter6th\n",
      "3 hours ago (edited)\n",
      " @May the Science be with You  I've played around with them. They  working good, the results are amazing but I got bored real quick.\n",
      "\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Roksana Sultana\n",
      "Roksana Sultana\n",
      "2 days ago\n",
      "Man these ai generated UI designs are sick af\n",
      "\n",
      "59\n",
      "\n",
      "DesignCourse\n",
      "\n",
      "Reply\n",
      "\n",
      "Darrel G\n",
      "Darrel G\n",
      "1 day ago\n",
      "this is actually a great use for this kind of technology. I've been using it  for inspiration overall for other art projects, but I didn't think to use it for my website/app design work\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "secession77\n",
      "secession77\n",
      "11 hours ago\n",
      "This is so cool for genuine asset generation. Sometimes when I have an idea I spent too much time on looking for the right photo/render. This might be a game changer for me.\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Reply\n",
      "\n",
      "Response: ```java\n",
      "public class MergeSort {\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "        int[] arr = {38, 27, 43, 3, 9, 82, 10};\n",
      "        System.out.println(\"Original array:\");\n",
      "        for (int num...\n",
      "Overall Score: 3.50\n",
      "Category Scores:\n",
      "  Relevance: 2\n",
      "  Accuracy: 4\n",
      "  Completeness: 3\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 9:\n",
      "Original Prompt: Can I provide you a bunch of data that would could then extract interesting statistics that would be informative for a business development manager or sales manager?\n",
      "Response: ```javascript\n",
      "class GraphicLayer {\n",
      "  constructor(parent, image, properties) {\n",
      "    this.parent = parent;\n",
      "    this.properties = properties || {};\n",
      "    this.img = null;\n",
      "\n",
      "\n",
      "    if (typeof image === ...\n",
      "Overall Score: 3.50\n",
      "Category Scores:\n",
      "  Relevance: 2\n",
      "  Accuracy: 5\n",
      "  Completeness: 2\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Response 10:\n",
      "Original Prompt: Please devise a multi-step plan for FenerbahÃ§e to become more successful than Galatasaray in football\n",
      "Response: ```python\n",
      "# Import necessary libraries\n",
      "import re  # For regular expression matching\n",
      "import random  # For fallback responses\n",
      "\n",
      "# Step 1: Define the intent of common customer inquiries\n",
      "# This is a ...\n",
      "Overall Score: 3.50\n",
      "Category Scores:\n",
      "  Relevance: 2\n",
      "  Accuracy: 5\n",
      "  Completeness: 2\n",
      "  Clarity: 5\n",
      "Strongest Category: Completeness\n",
      "Weakest Category: Accuracy\n",
      "\n",
      "Results saved to response_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../data/FineTunedResponses/responses_to_fine_tuned.csv').head(100)\n",
    "\n",
    "# Define batch parameters\n",
    "BATCH_SIZE = 20\n",
    "SLEEP_TIME = .2  # seconds between batches\n",
    "\n",
    "# Initialize empty list for all results\n",
    "all_batch_results = []\n",
    "\n",
    "# Process in batches\n",
    "for start_idx in range(0, len(df), BATCH_SIZE):\n",
    "    print(f\"\\nProcessing batch {start_idx//BATCH_SIZE + 1} of {(len(df) + BATCH_SIZE - 1)//BATCH_SIZE}\")\n",
    "    \n",
    "    # Get current batch\n",
    "    batch_df = df.iloc[start_idx:start_idx + BATCH_SIZE]\n",
    "    \n",
    "    # Prepare responses and prompts for this batch\n",
    "    batch_responses = batch_df['response_to_fine_tuned'].tolist()\n",
    "    batch_prompts = batch_df['original_prompt'].tolist()\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the batch\n",
    "        batch_results = evaluate_responses(batch_responses, batch_prompts)\n",
    "        all_batch_results.append(batch_results)\n",
    "        \n",
    "        # Print batch summary\n",
    "        print(f\"Batch best response score: {batch_results['best_response']['overall_score']:.2f}\")\n",
    "        print(f\"Batch average score: {batch_results['summary']['average_score']:.2f}\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        batch_results_df = pd.DataFrame([{\n",
    "            'original_id': batch_df.iloc[i]['original_id'],\n",
    "            'original_prompt': batch_df.iloc[i]['original_prompt'],\n",
    "            'response': result['response'],\n",
    "            'overall_score': result['overall_score'],\n",
    "            **{f'score_{category}': score for category, score in result['scores'].items()},\n",
    "            'strongest_category': result['strongest_category'],\n",
    "            'weakest_category': result['weakest_category']\n",
    "        } for i, result in enumerate(batch_results['all_results'])])\n",
    "        \n",
    "        batch_results_df.to_csv(f'response_evaluation_results_batch_{start_idx//BATCH_SIZE + 1}.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {start_idx//BATCH_SIZE + 1}: {str(e)}\")\n",
    "    \n",
    "    # Sleep between batches\n",
    "    time.sleep(SLEEP_TIME)\n",
    "\n",
    "# Combine all results\n",
    "all_results = []\n",
    "all_responses = []\n",
    "for batch_result in all_batch_results:\n",
    "    all_results.extend(batch_result['all_results'])\n",
    "    \n",
    "# Calculate overall statistics\n",
    "overall_results = {\n",
    "    'best_response': max(all_results, key=lambda x: x['overall_score']),\n",
    "    'all_results': all_results,\n",
    "    'summary': {\n",
    "        'average_score': sum(r['overall_score'] for r in all_results) / len(all_results),\n",
    "        'score_distribution': {\n",
    "            category: sum(r['scores'][category] for r in all_results) / len(all_results)\n",
    "            for category in all_results[0]['scores'].keys()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(\"\\nBest Overall Response:\")\n",
    "print(f\"Overall Score: {overall_results['best_response']['overall_score']:.2f}\")\n",
    "print(\"\\nCategory Scores:\")\n",
    "for category, score in overall_results['best_response']['scores'].items():\n",
    "    print(f\"{category}: {score}\")\n",
    "\n",
    "print(\"\\nOverall Summary:\")\n",
    "print(f\"Average Score: {overall_results['summary']['average_score']:.2f}\")\n",
    "print(\"\\nCategory Averages:\")\n",
    "for category, score in overall_results['summary']['score_distribution'].items():\n",
    "    print(f\"{category}: {score:.2f}\")\n",
    "\n",
    "# Save final combined results\n",
    "final_results_df = pd.DataFrame([{\n",
    "    'original_id': df.iloc[all_results.index(result)]['original_id'],\n",
    "    'original_prompt': df.iloc[all_results.index(result)]['original_prompt'],\n",
    "    'response': result['response'],\n",
    "    'overall_score': result['overall_score'],\n",
    "    **{f'score_{category}': score for category, score in result['scores'].items()},\n",
    "    'strongest_category': result['strongest_category'],\n",
    "    'weakest_category': result['weakest_category']\n",
    "} for result in all_results])\n",
    "\n",
    "final_results_df.to_csv('response_evaluation_results_final.csv', index=False)\n",
    "print(\"\\nFinal results saved to response_evaluation_results_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5659b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
