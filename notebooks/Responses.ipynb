{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from google.api_core import retry\n",
    "import os\n",
    "# Import environment variables from env.json\n",
    "import json\n",
    "\n",
    "# Load environment variables from env.json\n",
    "with open('../env.json', 'r') as f:\n",
    "    env_vars = json.load(f)\n",
    "# Set environment variables from the loaded file\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = env_vars[\"google_cloud_project\"]\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = env_vars[\"google_cloud_location\"]\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = env_vars[\"google_genai_use_vertexai\"]\n",
    "# Set the fine-tuned model ID as an environment variable\n",
    "os.environ[\"FINE_TUNED_MODEL_ID\"] = env_vars[\"fine_tuned_model_id\"]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = env_vars[\"google_api_keys\"][3]\n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n",
      "Being a better team leader is a continuous journey of learning and growth. Here's a breakdown of key areas to focus on, encompassing various aspects of leadership:\n",
      "\n",
      "**I. Building a Strong Foundation (Core Principles):**\n",
      "\n",
      "*   **Lead by Example (Be a Role Model):**\n",
      "    *   **Integrity and Ethics:** Always act with honesty, transparency, and fairness. Your team will notice and respect your commitment to doing the right thing.\n",
      "    *   **Work Ethic:** Demonstrate hard work, dedication, and a positive attitude. Be the first to arrive, last to leave, and willing to pitch in on tasks.\n",
      "    *   **Communication:** Model clear, concise, and respectful communication.\n",
      "    *   **Responsibility:** Take ownership of your mistakes and accept accountability. Don't shift blame.\n",
      "*   **Foster Trust and Psychological Safety:**\n",
      "    *   **Open Communication:** Encourage open dialogue, feedback, and the sharing of ideas. Listen actively and genuinely.\n",
      "    *   **Vulnerability:** Be willing to share your own challenges and mistakes to build trust and demonstrate authenticity.\n",
      "    *   **Respect:** Treat every team member with respect, regardless of their role or experience.\n",
      "    *   **Create a safe space for risk-taking:** Allow team members to experiment, make mistakes (and learn from them), and challenge the status quo without fear of ridicule or punishment.\n",
      "*   **Embrace Continuous Learning and Development:**\n",
      "    *   **Self-Reflection:** Regularly evaluate your leadership style, identify areas for improvement, and seek feedback from your team.\n",
      "    *   **Training and Development:** Invest in your own professional development (e.g., leadership courses, workshops, reading books on leadership).\n",
      "    *   **Mentorship:** Seek out mentors or experienced leaders who can offer guidance and support.\n",
      "    *   **Stay Updated:** Keep abreast of industry trends, new management techniques, and best practices.\n",
      "\n",
      "**II. Effective Communication:**\n",
      "\n",
      "*   **Clear and Concise Communication:**\n",
      "    *   **Define Expectations:** Clearly communicate goals, objectives, deadlines, and roles.\n",
      "    *   **Regular Updates:** Provide regular updates on progress, challenges, and successes.\n",
      "    *   **Avoid Jargon:** Use simple, understandable language.\n",
      "*   **Active Listening:**\n",
      "    *   **Pay Attention:** Focus completely on the speaker, both verbally and nonverbally.\n",
      "    *   **Ask Clarifying Questions:** Ensure you understand the message fully.\n",
      "    *   **Summarize and Paraphrase:** Confirm your understanding of the message.\n",
      "    *   **Provide Feedback:** Show you're engaged and that you value their input.\n",
      "*   **Different Communication Channels:**\n",
      "    *   **Choose the right medium:** Email, meetings, instant messaging, phone calls, etc., and consider the best channel based on the message and audience.\n",
      "    *   **Adapt to Preferences:** Be mindful of team members' communication preferences (e.g., some might prefer email over frequent meetings).\n",
      "*   **Feedback:**\n",
      "    *   **Give Regular Feedback:** Provide both positive and constructive feedback on performance.\n",
      "    *   **Specific and Actionable:** Focus on specific behaviors and provide suggestions for improvement.\n",
      "    *   **Timely:** Deliver feedback promptly after the relevant event or behavior.\n",
      "    *   **Two-way:** Encourage feedback from your team members about your leadership.\n",
      "\n",
      "**III. Team Motivation and Empowerment:**\n",
      "\n",
      "*   **Set Clear Goals and Objectives:**\n",
      "    *   **SMART Goals:** Ensure goals are Specific, Measurable, Achievable, Relevant, and Time-bound.\n",
      "    *   **Alignment:** Connect individual goals to the overall team and organizational objectives.\n",
      "*   **Recognize and Reward Achievements:**\n",
      "    *   **Acknowledge Contributions:** Regularly recognize and appreciate team members' efforts and accomplishments.\n",
      "    *   **Formal and Informal Recognition:** Use both formal (e.g., bonuses, promotions) and informal (e.g., praise, public acknowledgements) methods of recognition.\n",
      "*   **Empower Team Members:**\n",
      "    *   **Delegate Responsibilities:** Give team members autonomy and ownership of their work.\n",
      "    *   **Provide Resources and Support:** Equip team members with the tools, training, and resources they need to succeed.\n",
      "    *   **Encourage Decision-Making:** Allow team members to make decisions within their areas of responsibility.\n",
      "*   **Foster a Positive and Inclusive Work Environment:**\n",
      "    *   **Promote Teamwork:** Encourage collaboration and cooperation.\n",
      "    *   **Celebrate Successes:** Acknowledge milestones and celebrate team achievements.\n",
      "    *   **Address Conflicts:** Address conflicts constructively and fairly.\n",
      "    *   **Promote Diversity and Inclusion:** Create an environment where all team members feel valued and respected.\n",
      "\n",
      "**IV. Managing Performance and Productivity:**\n",
      "\n",
      "*   **Performance Management:**\n",
      "    *   **Regular Performance Reviews:** Conduct regular performance reviews to assess progress, provide feedback, and set goals.\n",
      "    *   **Performance Improvement Plans (PIPs):** Develop PIPs for team members who are underperforming.\n",
      "*   **Time Management and Prioritization:**\n",
      "    *   **Help team members manage their time:** Teach and model effective time management techniques.\n",
      "    *   **Prioritize Tasks:** Help the team focus on the most important tasks.\n",
      "    *   **Utilize Time-Tracking Tools:** Consider tools that help track and analyze the team's time.\n",
      "*   **Problem-Solving and Decision-Making:**\n",
      "    *   **Encourage Problem-Solving:** Create an environment where the team can brainstorm and find solutions.\n",
      "    *   **Effective Decision-Making:** Implement a clear process for decision-making.\n",
      "    *   **Conflict Resolution:** Be skilled at resolving conflicts within the team.\n",
      "*   **Effective Meetings:**\n",
      "    *   **Plan and Prepare:** Have a clear agenda and objectives for each meeting.\n",
      "    *   **Facilitate and Moderate:** Guide the meeting effectively, ensuring participation and staying on track.\n",
      "    *   **Follow-Up:** Document decisions and actions, and follow up on assigned tasks.\n",
      "\n",
      "**V. Strategic Thinking and Vision:**\n",
      "\n",
      "*   **Understand the Big Picture:**\n",
      "    *   **Company Goals:** Align the team's goals with the company's overall objectives.\n",
      "    *   **Industry Trends:** Understand industry trends and how they may impact the team.\n",
      "*   **Develop a Vision and Strategy:**\n",
      "    *   **Define a Clear Vision:** Create a compelling vision for the team's future.\n",
      "    *   **Develop a Strategy:** Develop a plan for achieving the vision, including goals, objectives, and action steps.\n",
      "*   **Adapt and Innovate:**\n",
      "    *   **Embrace Change:** Be open to change and encourage innovation within the team.\n",
      "    *   **Identify Opportunities:** Look for new opportunities to improve the team's performance.\n",
      "\n",
      "**VI. Ongoing Development:**\n",
      "\n",
      "*   **Seek Feedback:**\n",
      "    *   **360-Degree Feedback:** Regularly solicit feedback from your team members, peers, and superiors.\n",
      "    *   **Anonymous Feedback:** Utilize anonymous feedback mechanisms to encourage open and honest input.\n",
      "*   **Reflect and Adjust:**\n",
      "    *   **Regularly Reflect on Your Leadership Style:** Review your approach, identify areas for improvement, and adapt as needed.\n",
      "    *   **Be Flexible:** Adjust your leadership style based on the needs of the team and the situation.\n",
      "*   **Embrace Challenges as Learning Opportunities:**\n",
      "    *   **Learn from Mistakes:** Don't be afraid to make mistakes; view them as opportunities for growth and learning.\n",
      "    *   **Persistence:** Stay persistent in your efforts to improve as a leader.\n",
      "\n",
      "**In summary, becoming a better team leader is a continuous process. It requires self-awareness, a commitment to lifelong learning, and a genuine desire to help your team members succeed. By focusing on these key areas, you can create a more productive, engaged, and fulfilling work environment for everyone.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# List available models\n",
    "print(\"Available models:\")\n",
    "for m in genai.list_models():\n",
    "    print(m.name)\n",
    "\n",
    "# Use a specific model\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-lite-001')  # base untuned model\n",
    "\n",
    "# Generate content\n",
    "response = model.generate_content(\"How can I be a better team leader?\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're talking to a 5-year-old. Explain Artificial Intelligence (AI) in simple terms. Use analogies a child would understand, like toys, games, or animals. Focus on what AI can *do*, not how it works internally. Explain how AI is different from a regular toy. Keep the explanation concise (under 150 words) and make it fun. The output should be a short paragraph suitable for a child.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The genai module doesn't have a Client attribute, so we'll use the GenerativeModel directly\n",
    "# No need to create a client instance\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = f'''You are a prompt engineering expert that transforms simple \n",
    "    prompts into more effective versions. Analyze the input prompt and create\n",
    "      an improved version that includes specific details, context, desired output format, \n",
    "      and any relevant constraints. Make the prompt clear, specific, and designed to \n",
    "      generate high-quality responses.\n",
    "      \n",
    "      Input Prompt: {prompt}\n",
    "\n",
    "      Respond ONLY with the text of the improved prompt, without any explanations, \n",
    "      introductions, or additional commentary.\n",
    "      '''\n",
    "\n",
    "    response = model.generate_content(contents=inputs)\n",
    "    return response.text\n",
    "\n",
    "print(generate_response('Explain AI to me like im a little kid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 1000 rows\n",
      "\n",
      "Dataset columns:\n",
      "- original_id\n",
      "- original_prompt\n",
      "- context\n",
      "- prompt\n",
      "\n",
      "First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_id</th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140731</td>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121053</td>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37805</td>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116016</td>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>make a javascript class \"GraphicLayer\" which i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132819</td>\n",
       "      <td>!Please outline the steps to build an automate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please outline the steps to build an automated...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_id                                    original_prompt context  \\\n",
       "0       140731   One-pot vegetarian pasta recipes for busy nights     NaN   \n",
       "1       121053  We have the following blog content... what is ...     NaN   \n",
       "2        37805  how o sort element using merge sort technique ...     NaN   \n",
       "3       116016  make a javascript class \"GraphicLayer\" which i...     NaN   \n",
       "4       132819  !Please outline the steps to build an automate...     NaN   \n",
       "\n",
       "                                              prompt  \n",
       "0   One-pot vegetarian pasta recipes for busy nights  \n",
       "1  We have the following blog content... what is ...  \n",
       "2  how o sort element using merge sort technique ...  \n",
       "3  make a javascript class \"GraphicLayer\" which i...  \n",
       "4  Please outline the steps to build an automated...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ShareGPT dataset from CSV\n",
    "try:\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = \"../data/ShareGPT/testing_prompts_cleaned.csv\"\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df_sharegpt = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset loaded successfully with {len(df_sharegpt)} rows\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in df_sharegpt.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Display the first few rows of the dataset\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    display(df_sharegpt.head())\n",
    "\n",
    "    df_sharegpt.drop(columns=['context', 'prompt'], inplace=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '../data/ShareGPT/separated_prompts_clean.csv' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {str(e)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating improved prompts for each original prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts: 100%|██████████| 1000/1000 [35:50<00:00,  2.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 1000 improved prompts\n",
      "Final results saved to ../data/BaseModelResponses/improved_prompts.csv\n",
      "\n",
      "Sample of results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_prompt</th>\n",
       "      <th>base_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One-pot vegetarian pasta recipes for busy nights</td>\n",
       "      <td>Develop five unique and easy one-pot vegetaria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have the following blog content... what is ...</td>\n",
       "      <td>Analyze the provided blog content about choosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how o sort element using merge sort technique ...</td>\n",
       "      <td>Create a Java program that implements the Merg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     original_prompt  \\\n",
       "0   One-pot vegetarian pasta recipes for busy nights   \n",
       "1  We have the following blog content... what is ...   \n",
       "2  how o sort element using merge sort technique ...   \n",
       "\n",
       "                                       base_response  \n",
       "0  Develop five unique and easy one-pot vegetaria...  \n",
       "1  Analyze the provided blog content about choosi...  \n",
       "2  Create a Java program that implements the Merg...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the generate_response function to each original_prompt in the dataframe\n",
    "# This will create a new column 'base_response' with the improved prompts\n",
    "print(\"Generating improved prompts for each original prompt...\")\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to safely apply generate_response\n",
    "def safe_generate_response(prompt):\n",
    "    try:\n",
    "        return generate_response(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {str(e)[:100]}...\")\n",
    "        return \"Error generating response\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/BaseModelResponses\", exist_ok=True)\n",
    "\n",
    "# Initialize counter for saving\n",
    "counter = [0]\n",
    "total_rows = len(df_sharegpt)\n",
    "\n",
    "# Define a function to process each row and save every 100 rows\n",
    "def process_and_save(prompt):\n",
    "    response = safe_generate_response(prompt)\n",
    "    return response\n",
    "\n",
    "# Apply the function to each row in the dataframe with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Processing prompts\")\n",
    "df_sharegpt['base_response'] = df_sharegpt['original_prompt'].progress_apply(process_and_save)\n",
    "\n",
    "# Save the final complete dataset\n",
    "df_sharegpt.to_csv(\"../data/BaseModelResponses/improved_prompts.csv\", index=False)\n",
    "print(f\"Completed generating {total_rows} improved prompts\")\n",
    "print(f\"Final results saved to ../data/BaseModelResponses/improved_prompts.csv\")\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "display(df_sharegpt[['original_prompt', 'base_response']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "\n",
    "\n",
    "# Access the model ID from environment variables\n",
    "fine_tuned_model_id = os.environ[\"FINE_TUNED_MODEL_ID\"]\n",
    "\n",
    "\n",
    "# Initialize the GenAI client for Vertex AI\n",
    "client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"How can I be a better team leader?\"\n",
    "\n",
    "# Call your fine-tuned model\n",
    "response = client.models.generate_content(\n",
    "    model=fine_tuned_model_id,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works with no context, but you can do with context too\n",
    "def generate_response(prompt):\n",
    "    inputs = f'''You are a prompt engineering expert that transforms simple \n",
    "    prompts into more effective versions. Analyze the input prompt and create\n",
    "      an improved version that includes specific details, context, desired output format, \n",
    "      and any relevant constraints. Make the prompt clear, specific, and designed to \n",
    "      generate high-quality responses.\n",
    "      \n",
    "      Input Prompt: {prompt}\n",
    "\n",
    "      Respond ONLY with the text of the improved prompt, without any explanations, \n",
    "      introductions, or additional commentary.\n",
    "      '''\n",
    "\n",
    "    return client.models.generate_content(\n",
    "        model=fine_tuned_model_id,\n",
    "        contents=inputs).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ShareGPT dataset from CSV\n",
    "try:\n",
    "    # Define the path to the CSV file\n",
    "    csv_path = \"../data/ShareGPT/separated_prompts_clean.csv\"\n",
    "    \n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df_sharegpt = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Display basic information about the dataset\n",
    "    print(f\"Dataset loaded successfully with {len(df_sharegpt)} rows\")\n",
    "    print(\"\\nDataset columns:\")\n",
    "    for col in df_sharegpt.columns:\n",
    "        print(f\"- {col}\")\n",
    "    \n",
    "    # Display the first few rows of the dataset\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    display(df_sharegpt.head())\n",
    "\n",
    "    df_sharegpt.drop(columns=['context', 'prompt'], inplace=True)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at '../data/ShareGPT/separated_prompts_clean.csv' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the generate_response function to each original_prompt in the dataframe\n",
    "# This will create a new column 'base_response' with the improved prompts\n",
    "print(\"Generating improved prompts for each original prompt...\")\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to safely apply generate_response\n",
    "def safe_generate_response(prompt):\n",
    "    try:\n",
    "        return generate_response(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {str(e)[:100]}...\")\n",
    "        return \"Error generating response\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(\"../data/FineTunedResponses\", exist_ok=True)\n",
    "\n",
    "# Initialize counter for saving\n",
    "counter = [0]\n",
    "total_rows = len(df_sharegpt)\n",
    "\n",
    "# Define a function to process each row and save every 100 rows\n",
    "def process_and_save(prompt):\n",
    "    response = safe_generate_response(prompt)\n",
    "    counter[0] += 1\n",
    "    \n",
    "    # Save every 100 rows\n",
    "    if counter[0] % 100 == 0:\n",
    "        batch_num = counter[0] // 100\n",
    "        print(f\"Processed {counter[0]} rows. Saving batch {batch_num}...\")\n",
    "        df_sharegpt.to_csv(f\"../data/FineTunedResponses/responses_batch_{batch_num}.csv\", index=False)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Apply the function to each row in the dataframe with tqdm progress bar\n",
    "tqdm.pandas(desc=\"Processing prompts\")\n",
    "df_sharegpt['fine_tuned_response'] = df_sharegpt['original_prompt'].progress_apply(process_and_save)\n",
    "\n",
    "# Save the final complete dataset\n",
    "df_sharegpt.to_csv(\"../data/FineTunedResponses/all_responses.csv\", index=False)\n",
    "print(f\"Completed generating {total_rows} improved prompts\")\n",
    "print(f\"Final results saved to ../data/FineTunedResponses/all_responses.csv\")\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "display(df_sharegpt[['original_prompt', 'fine_tuned_response']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sharegpt['original_prompt'].iloc[0])\n",
    "\n",
    "print(generate_response(df_sharegpt['original_prompt'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random examples from the dataset\n",
    "import random\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Select 5 random indices from the dataframe\n",
    "random_indices = random.sample(range(len(df_sharegpt)), 5)\n",
    "\n",
    "print(\"\\nRandom examples of prompts and their fine-tuned responses:\")\n",
    "for idx in random_indices:\n",
    "    print(f\"\\n--- Example {idx} ---\")\n",
    "    print(f\"Original Prompt:\\n{df_sharegpt['original_prompt'].iloc[idx]}\")\n",
    "    print(f\"\\nFine-tuned Response:\\n{df_sharegpt['fine_tuned_response'].iloc[idx]}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Alternative display using pandas\n",
    "print(\"\\nRandom examples as a dataframe:\")\n",
    "display(df_sharegpt.loc[random_indices, ['original_prompt', 'fine_tuned_response']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned responses\n",
    "print(\"Loading fine-tuned responses...\")\n",
    "df_fine_tuned = pd.read_csv(\"../data/FineTunedResponses/all_responses.csv\")\n",
    "\n",
    "# Load the Flash 2.0 responses\n",
    "print(\"Loading Flash 2.0 responses...\")\n",
    "df_flash = pd.read_csv(\"../data/Flash2.0Responses/all_responses.csv\")\n",
    "\n",
    "# Join the datasets on original_id\n",
    "print(\"Joining datasets on original_id...\")\n",
    "df_combined = pd.merge(\n",
    "    df_fine_tuned, \n",
    "    df_flash,\n",
    "    on=\"original_id\",\n",
    "    suffixes=(\"_fine_tuned\", \"_flash\")\n",
    ")\n",
    "\n",
    "# Display information about the combined dataset\n",
    "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "print(\"\\nSample of combined results:\")\n",
    "display(df_combined[['original_id', 'original_prompt_fine_tuned', 'fine_tuned_response', 'base_response']].head(5))\n",
    "\n",
    "# Save the combined dataset\n",
    "df_combined.to_csv(\"../data/combined_responses.csv\", index=False)\n",
    "print(\"Combined dataset saved to ../data/combined_responses.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some readable examples from the combined dataset\n",
    "print(\"\\nReadable examples from the combined dataset:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Select different examples to display in a more readable format\n",
    "# Choose examples with interesting contrasts between fine-tuned and base responses\n",
    "sample_indices = [116016, 132819, 654]  # Different examples from the dataset\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    row_idx = df_combined.index[df_combined['original_id'] == idx][0] if idx in df_combined['original_id'].values else i\n",
    "    print(f\"\\n\\n--- Example {i+1} ---\")\n",
    "    print(f\"Original Prompt:\\n{df_combined['original_prompt_fine_tuned'].iloc[row_idx]}\")\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Fine-tuned Model Response:\\n{df_combined['fine_tuned_response'].iloc[row_idx]}\")\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Base Model Response:\\n{df_combined['base_response'].iloc[row_idx]}\")\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "\n",
    "# Get 5 random examples for additional variety\n",
    "random_indices = random.sample(range(len(df_combined)), 5)\n",
    "print(\"\\n\\nAdditional random examples:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    print(f\"\\n\\n--- Random Example {i+1} ---\")\n",
    "    print(f\"Original Prompt:\\n{df_combined['original_prompt_fine_tuned'].iloc[idx]}\")\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Fine-tuned Model Response:\\n{df_combined['fine_tuned_response'].iloc[idx]}\")\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Base Model Response:\\n{df_combined['base_response'].iloc[idx]}\")\n",
    "    print(\"\\n\" + \"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "def get_response(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Rename fine_tuned_response to fine_tuned_prompt in the all_prompts.csv file\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/FineTunedResponses/all_prompts.csv\")\n",
    "\n",
    "# Rename the column\n",
    "prompts_df = prompts_df.rename(columns={'fine_tuned_response': 'fine_tuned_prompt'})\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['response_to_fine_tuned'] = None\n",
    "\n",
    "# Process each prompt and get a response\n",
    "print(\"\\nProcessing fine-tuned prompts:\")\n",
    "for i in tqdm(range(len(prompts_df))):\n",
    "    try:\n",
    "        # Get the response for the fine-tuned prompt\n",
    "        response = get_response(prompts_df.loc[i, 'fine_tuned_prompt'])\n",
    "        prompts_df.loc[i, 'response_to_fine_tuned'] = response\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        prompts_df.loc[i, 'response_to_fine_tuned'] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Wait a bit longer if there's an error\n",
    "        time.sleep(2)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/FineTunedResponses/responses_to_fine_tuned.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/FineTunedResponses/responses_to_fine_tuned.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to fine-tuned prompts:\")\n",
    "print(prompts_df[['fine_tuned_prompt', 'response_to_fine_tuned']].head(3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BaseModelResponses prompts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 823/1000 [1:32:55<06:39,  2.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 823: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 824/1000 [1:32:57<06:30,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing prompt 824: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:53:15<00:00,  6.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responses saved to ../data/BaseModelResponses/responses.csv\n",
      "\n",
      "Sample responses to Flash2.0 prompts:\n",
      "                                     original_prompt  \\\n",
      "0   One-pot vegetarian pasta recipes for busy nights   \n",
      "1  We have the following blog content... what is ...   \n",
      "2  how o sort element using merge sort technique ...   \n",
      "\n",
      "                                            response  \n",
      "0  Okay, here are some delicious and easy one-pot...  \n",
      "1  Okay, based on the content provided, here's th...  \n",
      "2  ```java\\npublic class MergeSort {\\n\\n    // Ma...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the model\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')  # base untuned model\n",
    "\n",
    "def get_response(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Process the prompts from Flash2.0Responses\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/BaseModelResponses/improved_prompts.csv\")\n",
    "\n",
    "# Rename the columns to match our expected format\n",
    "prompts_df = prompts_df.rename(columns={'base_response': 'base_prompt'})\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['response'] = None\n",
    "\n",
    "# Process each prompt and get a response\n",
    "print(\"\\nProcessing BaseModelResponses prompts:\")\n",
    "for i in tqdm(range(len(prompts_df))):\n",
    "    try:\n",
    "        # Get the response for the base prompt\n",
    "        response = get_response(prompts_df.loc[i, 'original_prompt'])\n",
    "        prompts_df.loc[i, 'response'] = response\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        prompts_df.loc[i, 'response'] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Wait a bit longer if there's an error\n",
    "        time.sleep(2)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/BaseModelResponses/responses.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/BaseModelResponses/responses.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to Flash2.0 prompts:\")\n",
    "print(prompts_df[['original_prompt', 'response']].head(3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "def get_response(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Process the prompts from ShareGPT\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the prompts from the CSV file\n",
    "prompts_df = pd.read_csv(\"../data/ShareGPT/separated_prompts_clean.csv\")\n",
    "\n",
    "# Create a new column to store the responses\n",
    "prompts_df['og_response'] = None\n",
    "\n",
    "# Process each prompt and get a response\n",
    "print(\"\\nProcessing ShareGPT prompts:\")\n",
    "for i in tqdm(range(len(prompts_df))):\n",
    "    try:\n",
    "        # Get the response for the prompt\n",
    "        response = get_response(prompts_df.loc[i, 'original_prompt'])\n",
    "        prompts_df.loc[i, 'og_response'] = response\n",
    "        \n",
    "        # Add a small delay to avoid rate limiting\n",
    "        time.sleep(0.5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt {i}: {e}\")\n",
    "        prompts_df.loc[i, 'og_response'] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Wait a bit longer if there's an error\n",
    "        time.sleep(2)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "prompts_df.to_csv(\"../data/ShareGPT/og_responses.csv\", index=False)\n",
    "print(\"\\nResponses saved to ../data/ShareGPT/og_responses.csv\")\n",
    "\n",
    "# Display a few examples of the responses\n",
    "print(\"\\nSample responses to ShareGPT prompts:\")\n",
    "print(prompts_df[['original_prompt', 'og_response']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
