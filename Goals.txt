ImprovePrompts -> ShareGPT (preprocessed dataset)
Data/ShareGPT -> Unfiltered English Prompts (english questions)
Testing Prompts -> Testing set, prompt into LLM, used to test how well it improves the prompts
Basic Prompts responses -> Fed uninproved prompts into gemini and has response

notebook generated:
Prompt Training -> LLM Generated prompts, good vs bad, and categories

Responses Notebook: 
Fed it fine tuned model and base model and original prompts, puts these prompts and outputs a better prompt for each of them (prompt engineer)
Take responses and outputted responses of each into flash2.0 responses and fine tuned responses


Goal -> These prompts are what a general person would say. We want to finely pick through a data set, say how do we make it better. Do it for a general prompt instead of a LLM prompt.



Pipeline ->
Take ShareGPT data, preproccess and filter it out and get a good dataset, ask model to generate improved prompts of this dataset

Use Prompt-training dataset to be the template for distinguishing good vs bad prompts and categorize the way/method of calling

Responses, feed in all 3 models and output a better prompt for each of them, Take responses and compare with each other in a grader/rubric to see what ways to improve the most for the specific category. 